{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Observability Demonstration Notebook\n",
        "\n",
        "This notebook demonstrates the AI Observability Logging System, explains the theory behind how it works, and provides practical tests to validate monitoring capabilities.\n",
        "\n",
        "## What is AI Observability?\n",
        "\n",
        "AI Observability is the practice of monitoring, logging, and analyzing AI/LLM applications to understand:\n",
        "- **What** the model is doing (prompts, responses, tool calls)\n",
        "- **How well** it's performing (latency, token usage, costs)\n",
        "- **When** things go wrong (errors, failures, edge cases)\n",
        "- **Why** decisions were made (conversation context, tool usage patterns)\n",
        "\n",
        "Unlike traditional application logging, AI observability focuses specifically on:\n",
        "- Model interactions and responses\n",
        "- Tool/function calling patterns\n",
        "- Token usage and costs\n",
        "- Response quality metrics\n",
        "- Conversation flow tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports successful\n",
            "✓ Working directory: /Users/gr-mbp/Sites/deploying-ai/05_src/assignment_chat/notebooks\n",
            "✓ 05_src directory: /Users/gr-mbp/Sites/deploying-ai/05_src\n"
          ]
        }
      ],
      "source": [
        "# Setup: Import libraries and configure paths\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "# Add paths for imports\n",
        "_notebook_dir = Path.cwd()\n",
        "# Notebook is at: 05_src/assignment_chat/notebooks/\n",
        "# So parent.parent is 05_src, and parent is assignment_chat\n",
        "_05_src_dir = _notebook_dir.parent.parent\n",
        "_assignment_chat_dir = _notebook_dir.parent\n",
        "\n",
        "# Add to Python path\n",
        "if str(_05_src_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(_05_src_dir))\n",
        "if str(_assignment_chat_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(_assignment_chat_dir))\n",
        "\n",
        "# Import AI observability components\n",
        "from utils.ai_logger import get_ai_logger, LogCategory, LogSeverity, AILogger, set_ai_logger\n",
        "from ai_observability.storage import LogStorage\n",
        "\n",
        "# Import chat engine for testing\n",
        "from src.core.chat_engine import ChatEngine\n",
        "\n",
        "# For visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "print(\"✓ Imports successful\")\n",
        "print(f\"✓ Working directory: {_notebook_dir}\")\n",
        "print(f\"✓ 05_src directory: {_05_src_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AI Logger initialized: /Users/gr-mbp/Sites/deploying-ai/05_src/assignment_chat/notebooks/test_ai_logs.db\n",
            "✓ Log Storage initialized\n",
            "\n",
            "Current Log Statistics:\n",
            "  Total logs: 0\n",
            "  By category: {}\n",
            "  By severity: {}\n"
          ]
        }
      ],
      "source": [
        "# Configure AI Logger\n",
        "# Use a test database for this notebook\n",
        "test_db_path = str(_notebook_dir / \"test_ai_logs.db\")\n",
        "\n",
        "# Create a logger instance for testing\n",
        "test_logger = AILogger(\n",
        "    storage_path=test_db_path,\n",
        "    min_severity=LogSeverity.DEBUG,  # Log everything for demonstration\n",
        "    max_log_length=5000  # Truncate very long texts\n",
        ")\n",
        "\n",
        "# Set as global logger\n",
        "set_ai_logger(test_logger)\n",
        "\n",
        "# Initialize storage for querying\n",
        "storage = LogStorage(test_db_path, create_if_missing=True)\n",
        "\n",
        "print(f\"✓ AI Logger initialized: {test_db_path}\")\n",
        "print(f\"✓ Log Storage initialized\")\n",
        "\n",
        "# Display current statistics\n",
        "stats = storage.get_statistics()\n",
        "print(f\"\\nCurrent Log Statistics:\")\n",
        "print(f\"  Total logs: {stats['total_logs']}\")\n",
        "print(f\"  By category: {stats['by_category']}\")\n",
        "print(f\"  By severity: {stats['by_severity']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Theory - How AI Observability Works\n",
        "\n",
        "### 2.1 Logging Architecture\n",
        "\n",
        "Our AI observability system uses **structured logging** separate from application logging:\n",
        "\n",
        "**Key Components:**\n",
        "1. **Context Variables**: Thread-local conversation ID tracking using Python's `contextvars`\n",
        "2. **Structured Data**: All logs stored as structured entries with consistent fields\n",
        "3. **SQLite Storage**: File-based database with indexes for fast querying\n",
        "4. **Category System**: Different log types (prompt, response, tool_call, error, etc.)\n",
        "5. **Severity Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
        "\n",
        "**Why Separate from Application Logging?**\n",
        "- Application logs focus on system events (startup, errors, configuration)\n",
        "- AI logs focus on model interactions (prompts, responses, tool calls, costs)\n",
        "- Different query patterns and analysis needs\n",
        "- Different retention and privacy requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Categories Available:\n",
            "  - prompt\n",
            "  - response\n",
            "  - tool_call\n",
            "  - tool_result\n",
            "  - evaluation\n",
            "  - performance\n",
            "  - cost\n",
            "  - error\n",
            "  - guardrail\n",
            "  - model_config\n",
            "\n",
            "Severity Levels Available:\n",
            "  - DEBUG\n",
            "  - INFO\n",
            "  - WARNING\n",
            "  - ERROR\n",
            "  - CRITICAL\n",
            "\n",
            "Database Schema:\n",
            "  - id: Unique log entry ID\n",
            "  - timestamp: When the event occurred\n",
            "  - conversation_id: Links related events together\n",
            "  - category: Type of event (prompt, response, tool_call, etc.)\n",
            "  - severity: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
            "  - message: Human-readable description\n",
            "  - metadata: Additional structured data (JSON)\n",
            "  - model_name: Which model was used\n",
            "  - prompt_text: Full prompt (may be truncated)\n",
            "  - response_text: Full response (may be truncated)\n",
            "  - token_count_input: Input tokens used\n",
            "  - token_count_output: Output tokens generated\n",
            "  - latency_ms: Response time in milliseconds\n",
            "  - cost_usd: Estimated cost in USD\n",
            "  - tool_name: Tool that was called (if applicable)\n",
            "  - evaluation_scores: Quality metrics (if applicable)\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate the logging architecture\n",
        "print(\"Log Categories Available:\")\n",
        "for category in LogCategory:\n",
        "    print(f\"  - {category.value}\")\n",
        "\n",
        "print(\"\\nSeverity Levels Available:\")\n",
        "for severity in LogSeverity:\n",
        "    print(f\"  - {severity.value}\")\n",
        "\n",
        "# Show database schema\n",
        "print(\"\\nDatabase Schema:\")\n",
        "print(\"  - id: Unique log entry ID\")\n",
        "print(\"  - timestamp: When the event occurred\")\n",
        "print(\"  - conversation_id: Links related events together\")\n",
        "print(\"  - category: Type of event (prompt, response, tool_call, etc.)\")\n",
        "print(\"  - severity: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\")\n",
        "print(\"  - message: Human-readable description\")\n",
        "print(\"  - metadata: Additional structured data (JSON)\")\n",
        "print(\"  - model_name: Which model was used\")\n",
        "print(\"  - prompt_text: Full prompt (may be truncated)\")\n",
        "print(\"  - response_text: Full response (may be truncated)\")\n",
        "print(\"  - token_count_input: Input tokens used\")\n",
        "print(\"  - token_count_output: Output tokens generated\")\n",
        "print(\"  - latency_ms: Response time in milliseconds\")\n",
        "print(\"  - cost_usd: Estimated cost in USD\")\n",
        "print(\"  - tool_name: Tool that was called (if applicable)\")\n",
        "print(\"  - evaluation_scores: Quality metrics (if applicable)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 LangChain Tool Calling Theory\n",
        "\n",
        "**How LangChain Tool Calling Works:**\n",
        "\n",
        "1. **Tool Binding**: Tools are bound to the model using `model.bind_tools(tools)`\n",
        "   - Each tool has a name, description, and parameters schema\n",
        "   - LangChain converts tools to function calling format\n",
        "\n",
        "2. **Function Calling Flow**:\n",
        "   ```\n",
        "   User Prompt → Model → Decision: Use Tool? → Tool Execution → Model → Final Response\n",
        "   ```\n",
        "\n",
        "3. **Message Types**:\n",
        "   - **SystemMessage**: Instructions for the model (system prompt)\n",
        "   - **HumanMessage**: User input\n",
        "   - **AIMessage**: Model responses (may contain tool calls)\n",
        "   - **ToolMessage**: Results from tool execution\n",
        "\n",
        "4. **Tool Execution Lifecycle**:\n",
        "   - Model decides to call a tool based on prompt\n",
        "   - Tool is invoked with parameters\n",
        "   - Tool result is added as ToolMessage\n",
        "   - Model processes tool result and generates final response\n",
        "   - May require multiple tool calls in sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Tool Calling Flow:\n",
            "============================================================\n",
            "\n",
            "1. User sends message: \"What videos did I watch recently?\"\n",
            "   └─> HumanMessage created\n",
            "\n",
            "2. ChatEngine processes message:\n",
            "   ├─> Generates conversation_id (UUID)\n",
            "   ├─> Logs PROMPT event\n",
            "   └─> Invokes model_with_tools.invoke(messages)\n",
            "\n",
            "3. Model receives messages:\n",
            "   ├─> SystemMessage: Instructions\n",
            "   ├─> HumanMessage: User query\n",
            "   └─> Decides: \"I need to call get_recent_videos tool\"\n",
            "\n",
            "4. Model returns AIMessage with tool_calls:\n",
            "   └─> Tool: get_recent_videos, Args: {\"limit\": 10}\n",
            "\n",
            "5. ChatEngine handles tool call:\n",
            "   ├─> Logs TOOL_CALL event\n",
            "   ├─> Executes tool (calls YouTube API)\n",
            "   ├─> Logs TOOL_RESULT event (success/failure)\n",
            "   └─> Creates ToolMessage with result\n",
            "\n",
            "6. Model processes tool result:\n",
            "   ├─> Receives ToolMessage with video data\n",
            "   ├─> Generates natural language response\n",
            "   └─> Returns final AIMessage\n",
            "\n",
            "7. ChatEngine logs response:\n",
            "   ├─> Logs RESPONSE event\n",
            "   ├─> Records token counts, latency, cost\n",
            "   └─> Returns response to user\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate tool calling flow with a diagram\n",
        "print(\"LangChain Tool Calling Flow:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "1. User sends message: \"What videos did I watch recently?\"\n",
        "   └─> HumanMessage created\n",
        "\n",
        "2. ChatEngine processes message:\n",
        "   ├─> Generates conversation_id (UUID)\n",
        "   ├─> Logs PROMPT event\n",
        "   └─> Invokes model_with_tools.invoke(messages)\n",
        "\n",
        "3. Model receives messages:\n",
        "   ├─> SystemMessage: Instructions\n",
        "   ├─> HumanMessage: User query\n",
        "   └─> Decides: \"I need to call get_recent_videos tool\"\n",
        "\n",
        "4. Model returns AIMessage with tool_calls:\n",
        "   └─> Tool: get_recent_videos, Args: {\"limit\": 10}\n",
        "\n",
        "5. ChatEngine handles tool call:\n",
        "   ├─> Logs TOOL_CALL event\n",
        "   ├─> Executes tool (calls YouTube API)\n",
        "   ├─> Logs TOOL_RESULT event (success/failure)\n",
        "   └─> Creates ToolMessage with result\n",
        "\n",
        "6. Model processes tool result:\n",
        "   ├─> Receives ToolMessage with video data\n",
        "   ├─> Generates natural language response\n",
        "   └─> Returns final AIMessage\n",
        "\n",
        "7. ChatEngine logs response:\n",
        "   ├─> Logs RESPONSE event\n",
        "   ├─> Records token counts, latency, cost\n",
        "   └─> Returns response to user\n",
        "\"\"\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Chat Engine Operation\n",
        "\n",
        "**Request Flow in Detail:**\n",
        "\n",
        "```\n",
        "User Message\n",
        "    ↓\n",
        "Generate conversation_id (UUID)\n",
        "    ↓\n",
        "Set conversation context (contextvars)\n",
        "    ↓\n",
        "Log PROMPT event\n",
        "    ├─> prompt_text: User message\n",
        "    ├─> model_name: Which model\n",
        "    └─> conversation_id: Links all events\n",
        "    ↓\n",
        "Build message history\n",
        "    ├─> SystemMessage: Instructions\n",
        "    ├─> Previous messages (if any)\n",
        "    └─> Current HumanMessage\n",
        "    ↓\n",
        "Invoke model_with_tools\n",
        "    ├─> Start timer\n",
        "    └─> model.invoke(messages)\n",
        "    ↓\n",
        "Model Response\n",
        "    ├─> Extract token counts\n",
        "    ├─> Calculate latency\n",
        "    └─> Check for tool_calls\n",
        "    ↓\n",
        "[If tool_calls exist]\n",
        "    ├─> Log each TOOL_CALL\n",
        "    ├─> Execute each tool\n",
        "    ├─> Log each TOOL_RESULT\n",
        "    └─> Invoke model again with tool results\n",
        "    ↓\n",
        "Log RESPONSE event\n",
        "    ├─> response_text: Model output\n",
        "    ├─> token_count_input/output\n",
        "    ├─> latency_ms\n",
        "    └─> cost_usd (if available)\n",
        "    ↓\n",
        "Clear conversation context\n",
        "    ↓\n",
        "Return response to user\n",
        "```\n",
        "\n",
        "**Key Design Decisions:**\n",
        "- **Conversation ID**: Generated per request, links all related events\n",
        "- **Context Variables**: Thread-safe way to track conversation without passing IDs everywhere\n",
        "- **Structured Logging**: All events stored with consistent schema for easy querying\n",
        "- **Error Handling**: Errors are logged but don't break the flow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Observability Benefits\n",
        "\n",
        "**1. Cost Tracking**\n",
        "- Monitor token usage per request\n",
        "- Track costs over time\n",
        "- Identify expensive queries\n",
        "- Optimize prompts to reduce costs\n",
        "\n",
        "**2. Performance Monitoring**\n",
        "- Measure latency (time-to-first-token, total time)\n",
        "- Identify slow queries\n",
        "- Track tool execution times\n",
        "- Optimize for better user experience\n",
        "\n",
        "**3. Quality Assurance**\n",
        "- Track response quality\n",
        "- Monitor error rates\n",
        "- Detect hallucinations or inappropriate responses\n",
        "- Evaluate tool usage patterns\n",
        "\n",
        "**4. Debugging and Troubleshooting**\n",
        "- Trace conversation flow\n",
        "- Identify where errors occur\n",
        "- Understand model decision-making\n",
        "- Reproduce issues with conversation IDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Creating Test Events\n",
        "\n",
        "Let's create various test scenarios to demonstrate logging capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Basic Logging Tests\n",
        "\n",
        "#### Test 1: Simple Prompt/Response (No Tools)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test 1 Complete - Conversation ID: f9ad6de0-aecf-429a-b41f-9036a567b7cb\n",
            "  Prompt: Hello, how are you?\n",
            "  Response: I'm doing well, thank you for asking!\n",
            "\n",
            "  Logged 2 events:\n",
            "    - response: Response received from gpt-4o-mini\n",
            "    - prompt: Prompt sent to gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Test 1: Simple prompt/response logging\n",
        "ai_logger = get_ai_logger()\n",
        "conv_id_1 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_1)\n",
        "\n",
        "# Simulate a simple prompt/response\n",
        "prompt = \"Hello, how are you?\"\n",
        "response = \"I'm doing well, thank you for asking!\"\n",
        "\n",
        "# Log the prompt\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_1\n",
        ")\n",
        "\n",
        "# Simulate some processing time\n",
        "time.sleep(0.1)\n",
        "\n",
        "# Log the response\n",
        "ai_logger.log_response(\n",
        "    response_text=response,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=10,\n",
        "    token_count_output=8,\n",
        "    latency_ms=150.5,\n",
        "    conversation_id=conv_id_1\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Test 1 Complete - Conversation ID: {conv_id_1}\")\n",
        "print(f\"  Prompt: {prompt}\")\n",
        "print(f\"  Response: {response}\")\n",
        "\n",
        "# Query the logs\n",
        "logs = storage.get_conversation_logs(conv_id_1)\n",
        "print(f\"\\n  Logged {len(logs)} events:\")\n",
        "for log in logs:\n",
        "    print(f\"    - {log['category']}: {log['message']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test 2: Prompt with Tool Calls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test 2 Complete - Conversation ID: f57d96a8-5235-49eb-8594-2026d5d2d3d9\n",
            "\n",
            "  Logged 4 events:\n",
            "    - response: Response received from gpt-4o-mini\n",
            "    - tool_result: Tool get_recent_videos succeeded\n",
            "      Tool: get_recent_videos\n",
            "    - tool_call: Tool called: get_recent_videos\n",
            "      Tool: get_recent_videos\n",
            "    - prompt: Prompt sent to gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Test 2: Prompt with tool calls\n",
        "conv_id_2 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_2)\n",
        "\n",
        "prompt = \"What videos did I watch recently?\"\n",
        "tool_name = \"get_recent_videos\"\n",
        "tool_args = {\"limit\": 10}\n",
        "\n",
        "# Log prompt\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_2\n",
        ")\n",
        "\n",
        "# Log tool call\n",
        "ai_logger.log_tool_call(\n",
        "    tool_name=tool_name,\n",
        "    tool_args=tool_args,\n",
        "    conversation_id=conv_id_2\n",
        ")\n",
        "\n",
        "# Simulate tool execution\n",
        "time.sleep(0.2)\n",
        "\n",
        "# Log tool result\n",
        "ai_logger.log_tool_result(\n",
        "    tool_name=tool_name,\n",
        "    success=True,\n",
        "    latency_ms=200.0,\n",
        "    conversation_id=conv_id_2,\n",
        "    result_preview=\"Found 10 recent videos...\"\n",
        ")\n",
        "\n",
        "# Log final response\n",
        "ai_logger.log_response(\n",
        "    response_text=\"You've watched 10 videos recently. Here are some highlights...\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=45,\n",
        "    token_count_output=120,\n",
        "    latency_ms=850.5,\n",
        "    conversation_id=conv_id_2\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Test 2 Complete - Conversation ID: {conv_id_2}\")\n",
        "logs = storage.get_conversation_logs(conv_id_2)\n",
        "print(f\"\\n  Logged {len(logs)} events:\")\n",
        "for log in logs:\n",
        "    print(f\"    - {log['category']}: {log['message']}\")\n",
        "    if log.get('tool_name'):\n",
        "        print(f\"      Tool: {log['tool_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test 3: Multiple Tool Calls in Sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test 3 Complete - Conversation ID: d601cdef-f12e-4e9c-87da-6ababca78363\n",
            "  Prompt: Tell me about my watch statistics and recent videos\n",
            "\n",
            "  Logged 6 events, 2 tool calls:\n",
            "    - response: Response received from gpt-4o-mini\n",
            "      Tokens: 240 (in: 60, out: 180)\n",
            "      Latency: 1200.00ms\n",
            "    - tool_result: Tool get_recent_videos succeeded\n",
            "      Tool: get_recent_videos\n",
            "      Latency: 100.00ms\n",
            "    - tool_call: Tool called: get_recent_videos\n",
            "      Tool: get_recent_videos\n",
            "    - tool_result: Tool get_statistics succeeded\n",
            "      Tool: get_statistics\n",
            "      Latency: 150.00ms\n",
            "    - tool_call: Tool called: get_statistics\n",
            "      Tool: get_statistics\n",
            "    - prompt: Prompt sent to gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Test 3: Multiple tool calls\n",
        "conv_id_3 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_3)\n",
        "\n",
        "prompt = \"Tell me about my watch statistics and recent videos\"\n",
        "\n",
        "# Log prompt\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "\n",
        "# First tool call\n",
        "ai_logger.log_tool_call(\n",
        "    tool_name=\"get_statistics\",\n",
        "    tool_args={},\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "time.sleep(0.15)\n",
        "ai_logger.log_tool_result(\n",
        "    tool_name=\"get_statistics\",\n",
        "    success=True,\n",
        "    latency_ms=150.0,\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "\n",
        "# Second tool call\n",
        "ai_logger.log_tool_call(\n",
        "    tool_name=\"get_recent_videos\",\n",
        "    tool_args={\"limit\": 5},\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "time.sleep(0.1)\n",
        "ai_logger.log_tool_result(\n",
        "    tool_name=\"get_recent_videos\",\n",
        "    success=True,\n",
        "    latency_ms=100.0,\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "\n",
        "# Final response\n",
        "ai_logger.log_response(\n",
        "    response_text=\"Your watch statistics show... and here are your recent videos...\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=60,\n",
        "    token_count_output=180,\n",
        "    latency_ms=1200.0,\n",
        "    conversation_id=conv_id_3\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Test 3 Complete - Conversation ID: {conv_id_3}\")\n",
        "print(f\"  Prompt: {prompt}\")\n",
        "logs = storage.get_conversation_logs(conv_id_3)\n",
        "tool_calls = [log for log in logs if log['category'] == 'tool_call']\n",
        "print(f\"\\n  Logged {len(logs)} events, {len(tool_calls)} tool calls:\")\n",
        "for log in logs:\n",
        "    print(f\"    - {log['category']}: {log['message']}\")\n",
        "    if log.get('tool_name'):\n",
        "        print(f\"      Tool: {log['tool_name']}\")\n",
        "    if log.get('token_count_input') or log.get('token_count_output'):\n",
        "        total_tokens = log.get('token_count_input', 0) + log.get('token_count_output', 0)\n",
        "        print(f\"      Tokens: {total_tokens} (in: {log.get('token_count_input', 0)}, out: {log.get('token_count_output', 0)})\")\n",
        "    if log.get('latency_ms'):\n",
        "        print(f\"      Latency: {log.get('latency_ms', 0):.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test 4 Complete - Conversation ID: 10a747ad-4ba5-4c17-a5d7-53063eae3c2a\n",
            "\n",
            "  Logged 4 events, 2 errors\n"
          ]
        }
      ],
      "source": [
        "# Test 4: Tool execution error\n",
        "conv_id_4 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_4)\n",
        "\n",
        "prompt = \"Get video details for invalid_id\"\n",
        "\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_4\n",
        ")\n",
        "\n",
        "# Tool call that will fail\n",
        "ai_logger.log_tool_call(\n",
        "    tool_name=\"get_video_details\",\n",
        "    tool_args={\"video_id\": \"invalid_id\"},\n",
        "    conversation_id=conv_id_4\n",
        ")\n",
        "\n",
        "# Simulate error\n",
        "time.sleep(0.05)\n",
        "\n",
        "# Log failed tool result\n",
        "ai_logger.log_tool_result(\n",
        "    tool_name=\"get_video_details\",\n",
        "    success=False,\n",
        "    latency_ms=50.0,\n",
        "    conversation_id=conv_id_4,\n",
        "    error=\"Video not found\",\n",
        "    error_type=\"NotFoundError\"\n",
        ")\n",
        "\n",
        "# Log error event\n",
        "ai_logger.log(\n",
        "    category=LogCategory.ERROR,\n",
        "    message=\"Tool execution failed: Video not found\",\n",
        "    severity=LogSeverity.ERROR,\n",
        "    conversation_id=conv_id_4,\n",
        "    tool_name=\"get_video_details\",\n",
        "    error_type=\"NotFoundError\"\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Test 4 Complete - Conversation ID: {conv_id_4}\")\n",
        "logs = storage.get_conversation_logs(conv_id_4)\n",
        "errors = [log for log in logs if log['severity'] == 'ERROR']\n",
        "print(f\"\\n  Logged {len(logs)} events, {len(errors)} errors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test 5: Model Errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Test 5 Complete - Conversation ID: 88255a1f-1e50-4436-bfae-6e16616b3850\n",
            "\n",
            "  Logged 2 events\n",
            "    ERROR: Model invocation failed: Rate limit exceeded\n"
          ]
        }
      ],
      "source": [
        "# Test 5: Model error\n",
        "conv_id_5 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_5)\n",
        "\n",
        "prompt = \"This might cause an error\"\n",
        "\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_5\n",
        ")\n",
        "\n",
        "# Simulate model error\n",
        "ai_logger.log(\n",
        "    category=LogCategory.ERROR,\n",
        "    message=\"Model invocation failed: Rate limit exceeded\",\n",
        "    severity=LogSeverity.ERROR,\n",
        "    conversation_id=conv_id_5,\n",
        "    error_type=\"RateLimitError\",\n",
        "    error_message=\"API rate limit exceeded. Please try again later.\"\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Test 5 Complete - Conversation ID: {conv_id_5}\")\n",
        "logs = storage.get_conversation_logs(conv_id_5)\n",
        "print(f\"\\n  Logged {len(logs)} events\")\n",
        "for log in logs:\n",
        "    if log['severity'] == 'ERROR':\n",
        "        print(f\"    ERROR: {log['message']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Conversation Scenarios\n",
        "\n",
        "#### Scenario 1: Single-Turn Conversation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Scenario 1 Complete - Single turn conversation\n",
            "  Conversation ID: 88c88155-e13b-4714-b3e5-a5e7a35643ae\n",
            "  Events logged: 2\n"
          ]
        }
      ],
      "source": [
        "# Scenario 1: Single-turn conversation\n",
        "conv_id_s1 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_s1)\n",
        "\n",
        "# User asks a question\n",
        "user_message = \"What's my total watch time?\"\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=user_message,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_s1\n",
        ")\n",
        "\n",
        "# Model responds (after tool call)\n",
        "ai_logger.log_response(\n",
        "    response_text=\"Your total watch time is 1,250 hours across all videos.\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=15,\n",
        "    token_count_output=12,\n",
        "    latency_ms=320.0,\n",
        "    conversation_id=conv_id_s1\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Scenario 1 Complete - Single turn conversation\")\n",
        "logs = storage.get_conversation_logs(conv_id_s1)\n",
        "print(f\"  Conversation ID: {conv_id_s1}\")\n",
        "print(f\"  Events logged: {len(logs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scenario 2: Multi-Turn Conversation with Context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Scenario 2 Complete - Multi-turn conversation\n",
            "  Conversation ID: 3c023f23-7a3a-4220-b78e-7e3be76f3f72\n",
            "  Total events: 6\n",
            "  Turns: 3 prompts, 3 responses\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total events: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Turns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prompts, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(responses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m responses\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken_count_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken_count_output\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlog\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total events: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Turns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prompts, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(responses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m responses\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[43mlog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken_count_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken_count_output\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlog\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlogs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "# Scenario 2: Multi-turn conversation\n",
        "conv_id_s2 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_s2)\n",
        "\n",
        "# Turn 1\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=\"What videos did I watch this week?\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_s2,\n",
        "    history_length=0\n",
        ")\n",
        "ai_logger.log_response(\n",
        "    response_text=\"You watched 15 videos this week, including...\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=20,\n",
        "    token_count_output=45,\n",
        "    latency_ms=450.0,\n",
        "    conversation_id=conv_id_s2\n",
        ")\n",
        "\n",
        "# Turn 2 (follow-up)\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=\"What about last week?\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_s2,\n",
        "    history_length=1  # Previous turn in history\n",
        ")\n",
        "ai_logger.log_response(\n",
        "    response_text=\"Last week you watched 12 videos...\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=25,  # Includes previous context\n",
        "    token_count_output=38,\n",
        "    latency_ms=380.0,\n",
        "    conversation_id=conv_id_s2\n",
        ")\n",
        "\n",
        "# Turn 3 (another follow-up)\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=\"Which week had more watch time?\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_s2,\n",
        "    history_length=2\n",
        ")\n",
        "ai_logger.log_response(\n",
        "    response_text=\"This week had more watch time with 8.5 hours compared to 6.2 hours last week.\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=30,\n",
        "    token_count_output=20,\n",
        "    latency_ms=420.0,\n",
        "    conversation_id=conv_id_s2\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "print(f\"✓ Scenario 2 Complete - Multi-turn conversation\")\n",
        "logs = storage.get_conversation_logs(conv_id_s2)\n",
        "prompts = [log for log in logs if log['category'] == 'prompt']\n",
        "responses = [log for log in logs if log['category'] == 'response']\n",
        "print(f\"  Conversation ID: {conv_id_s2}\")\n",
        "print(f\"  Total events: {len(logs)}\")\n",
        "print(f\"  Turns: {len(prompts)} prompts, {len(responses)} responses\")\n",
        "print(f\"  Total tokens: {sum(log.get('token_count_input', 0) + log.get('token_count_output', 0) for log in logs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response length: 5026 chars\n",
            "  ✓ Truncation detected in response\n",
            "  Prompt length: 5026 chars\n",
            "  ✓ Truncation detected in prompt\n",
            "\n",
            "✓ Edge Case 1 Complete - Long text handling\n"
          ]
        }
      ],
      "source": [
        "# Edge Case 1: Long text truncation\n",
        "conv_id_e1 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_e1)\n",
        "\n",
        "# Create a very long prompt (longer than max_log_length)\n",
        "long_prompt = \"Tell me about \" + \"video \" * 2000  # Very long prompt\n",
        "long_response = \"Here is a detailed analysis: \" + \"data \" * 2000  # Very long response\n",
        "\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=long_prompt,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_e1\n",
        ")\n",
        "\n",
        "ai_logger.log_response(\n",
        "    response_text=long_response,\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=5000,\n",
        "    token_count_output=3000,\n",
        "    latency_ms=2500.0,\n",
        "    conversation_id=conv_id_e1\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "# Check if truncation occurred\n",
        "logs = storage.get_conversation_logs(conv_id_e1)\n",
        "for log in logs:\n",
        "    if log.get('prompt_text'):\n",
        "        prompt_len = len(log['prompt_text'])\n",
        "        print(f\"  Prompt length: {prompt_len} chars\")\n",
        "        if \"[truncated\" in log['prompt_text']:\n",
        "            print(f\"  ✓ Truncation detected in prompt\")\n",
        "    if log.get('response_text'):\n",
        "        response_len = len(log['response_text'])\n",
        "        print(f\"  Response length: {response_len} chars\")\n",
        "        if \"[truncated\" in log['response_text']:\n",
        "            print(f\"  ✓ Truncation detected in response\")\n",
        "\n",
        "print(f\"\\n✓ Edge Case 1 Complete - Long text handling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Edge Case 2: High Token Usage Scenario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  High token usage detected:\n",
            "    Input tokens: 8,000\n",
            "    Output tokens: 4,000\n",
            "    Total tokens: 12,000\n",
            "    Estimated cost: $0.1500\n",
            "    Latency: 5000.00ms\n",
            "\n",
            "✓ Edge Case 2 Complete - High token usage\n"
          ]
        }
      ],
      "source": [
        "# Edge Case 2: High token usage\n",
        "conv_id_e2 = str(uuid.uuid4())\n",
        "ai_logger.set_conversation_id(conv_id_e2)\n",
        "\n",
        "ai_logger.log_prompt(\n",
        "    prompt_text=\"Analyze all my watch history in detail\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    conversation_id=conv_id_e2\n",
        ")\n",
        "\n",
        "# Simulate high token usage\n",
        "ai_logger.log_response(\n",
        "    response_text=\"Based on your extensive watch history...\",\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    token_count_input=8000,  # Very high input tokens\n",
        "    token_count_output=4000,  # Very high output tokens\n",
        "    latency_ms=5000.0,  # 5 seconds\n",
        "    cost_usd=0.15,  # Estimated cost\n",
        "    conversation_id=conv_id_e2\n",
        ")\n",
        "\n",
        "ai_logger.clear_conversation_id()\n",
        "\n",
        "logs = storage.get_conversation_logs(conv_id_e2)\n",
        "for log in logs:\n",
        "    if log.get('token_count_input'):\n",
        "        total_tokens = log.get('token_count_input', 0) + log.get('token_count_output', 0)\n",
        "        cost = log.get('cost_usd', 0)\n",
        "        print(f\"  High token usage detected:\")\n",
        "        print(f\"    Input tokens: {log['token_count_input']:,}\")\n",
        "        print(f\"    Output tokens: {log['token_count_output']:,}\")\n",
        "        print(f\"    Total tokens: {total_tokens:,}\")\n",
        "        print(f\"    Estimated cost: ${cost:.4f}\")\n",
        "        print(f\"    Latency: {log.get('latency_ms', 0):.2f}ms\")\n",
        "\n",
        "print(f\"\\n✓ Edge Case 2 Complete - High token usage\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deploying-ai-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
