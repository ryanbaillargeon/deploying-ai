{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets_grassriots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document cleaning utilities from utils module\n",
    "import sys\n",
    "sys.path.append('../05_src')\n",
    "\n",
    "from utils.document_cleaning import (\n",
    "    clean_document_text,\n",
    "    evaluate_cleaning,\n",
    "    print_evaluation_report\n",
    ")\n",
    "\n",
    "# Import PyPDFLoader from langchain_community\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./Managing Oneself_Drucker_HBR.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Combine all page content into a single string\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3118792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document length: 51452 characters\n",
      "Cleaned document length: 50434 characters\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT CLEANING EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TEXT STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Character Count:\n",
      "  Original: 51,452\n",
      "  Cleaned:  50,434\n",
      "  Reduction: 1,018 (1.98%)\n",
      "\n",
      "Word Count:\n",
      "  Original: 8,670\n",
      "  Cleaned:  8,427\n",
      "\n",
      "Sentence Count:\n",
      "  Original: 578\n",
      "  Cleaned:  578\n",
      "\n",
      "Average Words per Sentence:\n",
      "  Original: 15.00\n",
      "  Cleaned:  14.58\n",
      "\n",
      "ðŸ”¤ WHITESPACE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Spaces:\n",
      "  Original: 7,759\n",
      "  Cleaned:  8,426\n",
      "  Reduced:  -667\n",
      "\n",
      "Multiple Consecutive Spaces:\n",
      "  Original: 5\n",
      "  Cleaned:  0\n",
      "  Reduced:  5\n",
      "\n",
      "Tab Characters:\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "Single Newlines (broken sentences):\n",
      "  Original: 1,442\n",
      "  Cleaned:  0\n",
      "  Fixed:    1,442\n",
      "\n",
      "Excessive Blank Lines (>2 consecutive):\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "âœ¨ TEXT QUALITY INDICATORS\n",
      "--------------------------------------------------------------------------------\n",
      "Text Density (non-whitespace ratio):\n",
      "  Original: 0.8212\n",
      "  Cleaned:  0.8329\n",
      "  Improvement: Higher is better - more content, less whitespace\n",
      "\n",
      "ðŸŽ¯ LLM PROCESSING EFFICIENCY\n",
      "--------------------------------------------------------------------------------\n",
      "Token Count (approximate):\n",
      "  Original: 12,138\n",
      "  Cleaned:  10,523\n",
      "  Reduction: 1,615 (13.31%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "âœ… Characters reduced: 1,018 (1.98%)\n",
      "âœ… Tokens reduced: 1,615 (13.31%)\n",
      "âœ… Broken sentences fixed: 1,442\n",
      "âœ… Multiple spaces normalized: 5\n",
      "âœ… Tabs converted to spaces: 0\n",
      "âœ… Excessive blank lines removed: 0\n",
      "\n",
      "ðŸ’¡ The cleaned text uses 13.31% fewer tokens,\n",
      "   which means lower processing costs and faster LLM responses!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Note: All cleaning and evaluation functions are now imported from utils.document_cleaning module\n",
    "# Functions are available: clean_document_text, evaluate_cleaning, print_evaluation_report\n",
    "\n",
    "# I wanted to explore the impact of the cleaning process on the document length - and how it affects the LLM's ability to summarize the document.\n",
    "# Specifically, how we can optimize the amount of input tokens for the LLM, while still maintaining the quality of the summary.\n",
    "\n",
    "# Store original text before cleaning for comparison\n",
    "original_document_text = document_text\n",
    "\n",
    "# Clean the document text before processing with LLM\n",
    "document_text = clean_document_text(document_text)\n",
    "\n",
    "print(f\"Original document length: {len(original_document_text)} characters\")\n",
    "print(f\"Cleaned document length: {len(document_text)} characters\")\n",
    "print()\n",
    "\n",
    "# Evaluate the cleaning effectiveness\n",
    "cleaning_metrics = evaluate_cleaning(original_document_text, document_text)\n",
    "print_evaluation_report(cleaning_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'SummarySchema': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Missing 'InputTokens'.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     54\u001b[39m client = OpenAI()\n\u001b[32m     56\u001b[39m schema = SummarySchema.model_json_schema()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarySchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mschema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 5) Parse JSON text with Pydantic and add token usage\u001b[39;00m\n\u001b[32m     76\u001b[39m raw_json = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mcreate\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m    **Starting a new project?** We recommend trying\u001b[39;00m\n\u001b[32m    866\u001b[39m \u001b[33;03m    [Responses](https://platform.openai.com/docs/api-reference/responses) to take\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1106\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m   1107\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1108\u001b[39m     ...\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1113\u001b[39m     *,\n\u001b[32m   1114\u001b[39m     messages: Iterable[ChatCompletionMessageParam],\n\u001b[32m   1115\u001b[39m     model: Union[\u001b[38;5;28mstr\u001b[39m, ChatModel],\n\u001b[32m   1116\u001b[39m     audio: Optional[ChatCompletionAudioParam] | Omit = omit,\n\u001b[32m   1117\u001b[39m     frequency_penalty: Optional[\u001b[38;5;28mfloat\u001b[39m] | Omit = omit,\n\u001b[32m   1118\u001b[39m     function_call: completion_create_params.FunctionCall | Omit = omit,\n\u001b[32m   1119\u001b[39m     functions: Iterable[completion_create_params.Function] | Omit = omit,\n\u001b[32m   1120\u001b[39m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]] | Omit = omit,\n\u001b[32m   1121\u001b[39m     logprobs: Optional[\u001b[38;5;28mbool\u001b[39m] | Omit = omit,\n\u001b[32m   1122\u001b[39m     max_completion_tokens: Optional[\u001b[38;5;28mint\u001b[39m] | Omit = omit,\n\u001b[32m   1123\u001b[39m     max_tokens: Optional[\u001b[38;5;28mint\u001b[39m] | Omit = omit,\n\u001b[32m   1124\u001b[39m     metadata: Optional[Metadata] | Omit = omit,\n\u001b[32m   1125\u001b[39m     modalities: Optional[List[Literal[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m]]] | Omit = omit,\n\u001b[32m   1126\u001b[39m     n: Optional[\u001b[38;5;28mint\u001b[39m] | Omit = omit,\n\u001b[32m   1127\u001b[39m     parallel_tool_calls: \u001b[38;5;28mbool\u001b[39m | Omit = omit,\n\u001b[32m   1128\u001b[39m     prediction: Optional[ChatCompletionPredictionContentParam] | Omit = omit,\n\u001b[32m   1129\u001b[39m     presence_penalty: Optional[\u001b[38;5;28mfloat\u001b[39m] | Omit = omit,\n\u001b[32m   1130\u001b[39m     prompt_cache_key: \u001b[38;5;28mstr\u001b[39m | Omit = omit,\n\u001b[32m   1131\u001b[39m     reasoning_effort: Optional[ReasoningEffort] | Omit = omit,\n\u001b[32m   1132\u001b[39m     response_format: completion_create_params.ResponseFormat | Omit = omit,\n\u001b[32m   1133\u001b[39m     safety_identifier: \u001b[38;5;28mstr\u001b[39m | Omit = omit,\n\u001b[32m   1134\u001b[39m     seed: Optional[\u001b[38;5;28mint\u001b[39m] | Omit = omit,\n\u001b[32m   1135\u001b[39m     service_tier: Optional[Literal[\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflex\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpriority\u001b[39m\u001b[33m\"\u001b[39m]] | Omit = omit,\n\u001b[32m   1136\u001b[39m     stop: Union[Optional[\u001b[38;5;28mstr\u001b[39m], SequenceNotStr[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m] | Omit = omit,\n\u001b[32m   1137\u001b[39m     store: Optional[\u001b[38;5;28mbool\u001b[39m] | Omit = omit,\n\u001b[32m   1138\u001b[39m     stream: Optional[Literal[\u001b[38;5;28;01mFalse\u001b[39;00m]] | Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = omit,\n\u001b[32m   1139\u001b[39m     stream_options: Optional[ChatCompletionStreamOptionsParam] | Omit = omit,\n\u001b[32m   1140\u001b[39m     temperature: Optional[\u001b[38;5;28mfloat\u001b[39m] | Omit = omit,\n\u001b[32m   1141\u001b[39m     tool_choice: ChatCompletionToolChoiceOptionParam | Omit = omit,\n\u001b[32m   1142\u001b[39m     tools: Iterable[ChatCompletionToolUnionParam] | Omit = omit,\n\u001b[32m   1143\u001b[39m     top_logprobs: Optional[\u001b[38;5;28mint\u001b[39m] | Omit = omit,\n\u001b[32m   1144\u001b[39m     top_p: Optional[\u001b[38;5;28mfloat\u001b[39m] | Omit = omit,\n\u001b[32m   1145\u001b[39m     user: \u001b[38;5;28mstr\u001b[39m | Omit = omit,\n\u001b[32m   1146\u001b[39m     verbosity: Optional[Literal[\u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m]] | Omit = omit,\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     web_search_options: completion_create_params.WebSearchOptions | Omit = omit,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;66;03m# Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\u001b[39;00m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# The extra values given here take precedence over values defined on the client or passed to this method.\u001b[39;00m\n\u001b[32m   1150\u001b[39m     extra_headers: Headers | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1151\u001b[39m     extra_query: Query | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1152\u001b[39m     extra_body: Body | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m   1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1158\u001b[39m         body=maybe_transform(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m   1205\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'SummarySchema': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Missing 'InputTokens'.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "# --- Structured Summarization with OpenAI Chat Completions API ---\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# 1) Output Schema - ALL fields required for strict mode\n",
    "class SummarySchema(BaseModel):\n",
    "    model_config = ConfigDict(\n",
    "        extra='forbid',\n",
    "        json_schema_extra={'additionalProperties': False}\n",
    "    )\n",
    "    \n",
    "    Author: str = Field(..., description=\"Author of the article\")\n",
    "    Title: str = Field(..., description=\"Title of the article\")\n",
    "    Relevance: str = Field(..., description=\"One paragraph on relevance to AI professionals\")\n",
    "    Summary: str = Field(..., description=\"Concise summary, <=1000 tokens\")\n",
    "    Tone: str = Field(..., description=\"The tone used to produce the summary\")\n",
    "    # Remove Optional - make these required with placeholder values\n",
    "    InputTokens: int = Field(default=0, description=\"Filled from response.usage\")\n",
    "    OutputTokens: int = Field(default=0, description=\"Filled from response.usage\")\n",
    "\n",
    "# 2) Model + Tone\n",
    "MODEL_CANDIDATES = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4-turbo\",\n",
    "    \"o4-mini\",\n",
    "]\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # NOT GPT-5 family\n",
    "TONE = \"Skeptical and Cynical\"\n",
    "\n",
    "# 3) Prompts (separated) + dynamic context injection\n",
    "instructions = (\n",
    "    \"You are an information extraction and summarization assistant. \"\n",
    "    \"Return output STRICTLY matching the provided JSON schema. \"\n",
    "    \"Do not add fields. Do not include extra commentary. \"\n",
    "    \"Write the Summary in the specified Tone and keep it under 1000 tokens. \"\n",
    "    \"Use only facts from the provided document; avoid speculation or hallucinations.\"\n",
    ")\n",
    "\n",
    "user_template = (\n",
    "    \"Task: Extract metadata and summarize the document in the specified tone.\\n\"\n",
    "    \"- Tone: {tone}\\n\"\n",
    "    \"- Fields to fill: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens), Tone.\\n\"\n",
    "    \"For InputTokens and OutputTokens, use placeholder value 0 (will be overwritten).\\n\"\n",
    "    \"Document follows between <<< >>>. Use only its content.\\n\"\n",
    "    \"<<<\\n{context}\\n>>>\"\n",
    ")\n",
    "user_content = user_template.format(tone=TONE, context=document_text)\n",
    "\n",
    "# 4) Call the model with structured output via Chat Completions API\n",
    "client = OpenAI()\n",
    "\n",
    "schema = SummarySchema.model_json_schema()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"SummarySchema\",\n",
    "            \"schema\": schema,\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    },\n",
    "    max_completion_tokens=1200,\n",
    ")\n",
    "\n",
    "# 5) Parse JSON text with Pydantic and add token usage\n",
    "raw_json = response.choices[0].message.content\n",
    "\n",
    "parsed = SummarySchema.model_validate_json(raw_json)\n",
    "\n",
    "result = parsed.model_copy(update={\n",
    "    \"Tone\": TONE,\n",
    "    \"InputTokens\": response.usage.prompt_tokens,\n",
    "    \"OutputTokens\": response.usage.completion_tokens,\n",
    "})\n",
    "\n",
    "print(\"Token usage:\", response.usage)\n",
    "print(result.model_dump_json(indent=2))\n",
    "\n",
    "# 6) Sanity checks\n",
    "for field in [\"Author\", \"Title\", \"Relevance\", \"Summary\", \"Tone\"]:\n",
    "    assert getattr(result, field) and isinstance(getattr(result, field), str), f\"{field} missing/invalid\"\n",
    "assert result.Relevance.count(\".\") <= 6, \"Relevance seems longer than a short paragraph\"\n",
    "assert isinstance(result.InputTokens, int) and isinstance(result.OutputTokens, int), \"Token counts missing\"\n",
    "assert result.Tone == TONE, \"Tone label not preserved\"\n",
    "\n",
    "# 7) Save JSON\n",
    "summary_json_path = \"assignment_1_summary.json\"\n",
    "with open(summary_json_path, \"w\") as f:\n",
    "    f.write(result.model_dump_json(indent=2))\n",
    "summary_json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
