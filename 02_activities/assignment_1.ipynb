{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd91015",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0be641",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24bc0c",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e80ea",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8b62b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets_grassriots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e97f1",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8bb09e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS: Text Cleaning Helper Functions for LLM Processing.\n",
    "import re\n",
    "def clean_document_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize document text for LLM processing.\n",
    "    \n",
    "    This function applies basic data cleaning techniques to prepare PDF-extracted\n",
    "    text for optimal LLM processing. It handles common issues like encoding errors,\n",
    "    excessive whitespace, and improper line breaks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw document text extracted from PDF\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and normalized text ready for LLM processing\n",
    "    \"\"\"\n",
    "    # Step 1: Handle encoding issues gracefully\n",
    "    # Remove or replace problematic characters that may cause encoding errors\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # Handle common encoding issues by removing problematic unicode characters\n",
    "    text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # Step 2: Normalize line breaks - preserve paragraph breaks but join broken sentences\n",
    "    # Replace double newlines (paragraph breaks) with a temporary marker\n",
    "    text = text.replace('\\n\\n', '|||PARAGRAPH_BREAK|||')\n",
    "    # Replace single newlines with spaces (these are likely broken sentences)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Restore paragraph breaks\n",
    "    text = text.replace('|||PARAGRAPH_BREAK|||', '\\n\\n')\n",
    "    \n",
    "    # Step 3: Normalize whitespace\n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Step 4: Clean up hyphenated line breaks (common in PDFs)\n",
    "    # Fix words broken across lines with hyphens followed by space (e.g., \"word- \\nword\" -> \"wordword\")\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Step 5: Remove leading and trailing whitespace from each line\n",
    "    lines = text.split('\\n')\n",
    "    lines = [line.strip() for line in lines]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    # Step 6: Remove excessive blank lines (more than 2 consecutive newlines)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Step 7: Final trim of leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a941fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS: Text Cleaning Evaluation Helper Functions - DO NOT MODIFY\n",
    "try:\n",
    "    import tiktoken\n",
    "    TOKENIZER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TOKENIZER_AVAILABLE = False\n",
    "    print(\"Note: tiktoken not available. Using word-based token approximation.\")\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Count tokens in text using tiktoken if available, otherwise approximate.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to count tokens for\n",
    "        model (str): Model name for tokenizer (default: gpt-4o-mini)\n",
    "        \n",
    "    Returns:\n",
    "        int: Approximate token count\n",
    "    \"\"\"\n",
    "    if TOKENIZER_AVAILABLE:\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "            return len(encoding.encode(text))\n",
    "        except:\n",
    "            # Fallback to word-based approximation\n",
    "            return len(text.split()) // 0.75  # Rough approximation: ~0.75 words per token\n",
    "    else:\n",
    "        # Simple approximation: average English word is ~1.3 tokens\n",
    "        return int(len(text.split()) * 1.3)\n",
    "\n",
    "def evaluate_cleaning(original_text, cleaned_text):\n",
    "    \"\"\"\n",
    "    Evaluate the effectiveness of document cleaning by comparing original vs cleaned text.\n",
    "    \n",
    "    This function computes quantitative metrics including text statistics, whitespace\n",
    "    reduction, and token efficiency to demonstrate cleaning effectiveness.\n",
    "    \n",
    "    Args:\n",
    "        original_text (str): Original text before cleaning\n",
    "        cleaned_text (str): Text after cleaning\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Helper function to count patterns\n",
    "    def count_pattern(text, pattern):\n",
    "        return len(re.findall(pattern, text))\n",
    "    \n",
    "    # Helper function to count_multiple_spaces\n",
    "    def count_multiple_spaces(text):\n",
    "        return len(re.findall(r' {2,}', text))\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    metrics = {\n",
    "        'original': {},\n",
    "        'cleaned': {},\n",
    "        'improvements': {}\n",
    "    }\n",
    "    \n",
    "    # Character and word counts\n",
    "    metrics['original']['char_count'] = len(original_text)\n",
    "    metrics['cleaned']['char_count'] = len(cleaned_text)\n",
    "    metrics['improvements']['char_reduction'] = metrics['original']['char_count'] - metrics['cleaned']['char_count']\n",
    "    metrics['improvements']['char_reduction_pct'] = (metrics['improvements']['char_reduction'] / metrics['original']['char_count'] * 100) if metrics['original']['char_count'] > 0 else 0\n",
    "    \n",
    "    metrics['original']['word_count'] = len(original_text.split())\n",
    "    metrics['cleaned']['word_count'] = len(cleaned_text.split())\n",
    "    \n",
    "    # Whitespace metrics\n",
    "    metrics['original']['spaces'] = original_text.count(' ')\n",
    "    metrics['cleaned']['spaces'] = cleaned_text.count(' ')\n",
    "    \n",
    "    metrics['original']['tabs'] = original_text.count('\\t')\n",
    "    metrics['cleaned']['tabs'] = cleaned_text.count('\\t')\n",
    "    \n",
    "    metrics['original']['newlines'] = original_text.count('\\n')\n",
    "    metrics['cleaned']['newlines'] = cleaned_text.count('\\n')\n",
    "    \n",
    "    metrics['original']['multiple_spaces'] = count_multiple_spaces(original_text)\n",
    "    metrics['cleaned']['multiple_spaces'] = count_multiple_spaces(cleaned_text)\n",
    "    \n",
    "    # Count excessive blank lines (>2 consecutive)\n",
    "    metrics['original']['excessive_blank_lines'] = count_pattern(original_text, r'\\n{3,}')\n",
    "    metrics['cleaned']['excessive_blank_lines'] = count_pattern(cleaned_text, r'\\n{3,}')\n",
    "    \n",
    "    # Count single newlines (likely broken sentences)\n",
    "    single_newlines_original = count_pattern(original_text, r'(?<!\\n)\\n(?!\\n)')\n",
    "    single_newlines_cleaned = count_pattern(cleaned_text, r'(?<!\\n)\\n(?!\\n)')\n",
    "    metrics['original']['single_newlines'] = single_newlines_original\n",
    "    metrics['cleaned']['single_newlines'] = single_newlines_cleaned\n",
    "    \n",
    "    # Text density (non-whitespace ratio)\n",
    "    metrics['original']['text_density'] = len(re.sub(r'\\s', '', original_text)) / len(original_text) if len(original_text) > 0 else 0\n",
    "    metrics['cleaned']['text_density'] = len(re.sub(r'\\s', '', cleaned_text)) / len(cleaned_text) if len(cleaned_text) > 0 else 0\n",
    "    \n",
    "    # Sentence count (approximate)\n",
    "    metrics['original']['sentence_count'] = len(re.findall(r'[.!?]+', original_text))\n",
    "    metrics['cleaned']['sentence_count'] = len(re.findall(r'[.!?]+', cleaned_text))\n",
    "    \n",
    "    # Average words per sentence\n",
    "    metrics['original']['avg_words_per_sentence'] = metrics['original']['word_count'] / metrics['original']['sentence_count'] if metrics['original']['sentence_count'] > 0 else 0\n",
    "    metrics['cleaned']['avg_words_per_sentence'] = metrics['cleaned']['word_count'] / metrics['cleaned']['sentence_count'] if metrics['cleaned']['sentence_count'] > 0 else 0\n",
    "    \n",
    "    # Token counts\n",
    "    metrics['original']['token_count'] = count_tokens(original_text)\n",
    "    metrics['cleaned']['token_count'] = count_tokens(cleaned_text)\n",
    "    metrics['improvements']['token_reduction'] = metrics['original']['token_count'] - metrics['cleaned']['token_count']\n",
    "    metrics['improvements']['token_reduction_pct'] = (metrics['improvements']['token_reduction'] / metrics['original']['token_count'] * 100) if metrics['original']['token_count'] > 0 else 0\n",
    "    \n",
    "    # Calculate improvements\n",
    "    metrics['improvements']['spaces_reduced'] = metrics['original']['spaces'] - metrics['cleaned']['spaces']\n",
    "    metrics['improvements']['tabs_removed'] = metrics['original']['tabs'] - metrics['cleaned']['tabs']\n",
    "    metrics['improvements']['single_newlines_fixed'] = metrics['original']['single_newlines'] - metrics['cleaned']['single_newlines']\n",
    "    metrics['improvements']['multiple_spaces_reduced'] = metrics['original']['multiple_spaces'] - metrics['cleaned']['multiple_spaces']\n",
    "    metrics['improvements']['excessive_blank_lines_removed'] = metrics['original']['excessive_blank_lines'] - metrics['cleaned']['excessive_blank_lines']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_evaluation_report(metrics):\n",
    "    \"\"\"\n",
    "    Print a formatted evaluation report showing cleaning effectiveness.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Metrics dictionary from evaluate_cleaning()\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DOCUMENT CLEANING EVALUATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Text Statistics\n",
    "    print(\"ðŸ“Š TEXT STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Character Count:\")\n",
    "    print(f\"  Original: {metrics['original']['char_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['char_count']:,}\")\n",
    "    print(f\"  Reduction: {metrics['improvements']['char_reduction']:,} ({metrics['improvements']['char_reduction_pct']:.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Word Count:\")\n",
    "    print(f\"  Original: {metrics['original']['word_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['word_count']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Sentence Count:\")\n",
    "    print(f\"  Original: {metrics['original']['sentence_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['sentence_count']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Average Words per Sentence:\")\n",
    "    print(f\"  Original: {metrics['original']['avg_words_per_sentence']:.2f}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['avg_words_per_sentence']:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Whitespace Metrics\n",
    "    print(\"ðŸ”¤ WHITESPACE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Spaces:\")\n",
    "    print(f\"  Original: {metrics['original']['spaces']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['spaces']:,}\")\n",
    "    print(f\"  Reduced:  {metrics['improvements']['spaces_reduced']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Multiple Consecutive Spaces:\")\n",
    "    print(f\"  Original: {metrics['original']['multiple_spaces']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['multiple_spaces']:,}\")\n",
    "    print(f\"  Reduced:  {metrics['improvements']['multiple_spaces_reduced']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Tab Characters:\")\n",
    "    print(f\"  Original: {metrics['original']['tabs']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['tabs']:,}\")\n",
    "    print(f\"  Removed:  {metrics['improvements']['tabs_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Single Newlines (broken sentences):\")\n",
    "    print(f\"  Original: {metrics['original']['single_newlines']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['single_newlines']:,}\")\n",
    "    print(f\"  Fixed:    {metrics['improvements']['single_newlines_fixed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Excessive Blank Lines (>2 consecutive):\")\n",
    "    print(f\"  Original: {metrics['original']['excessive_blank_lines']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['excessive_blank_lines']:,}\")\n",
    "    print(f\"  Removed:  {metrics['improvements']['excessive_blank_lines_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Text Quality\n",
    "    print(\"âœ¨ TEXT QUALITY INDICATORS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Text Density (non-whitespace ratio):\")\n",
    "    print(f\"  Original: {metrics['original']['text_density']:.4f}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['text_density']:.4f}\")\n",
    "    print(f\"  Improvement: {'Higher is better - more content, less whitespace' if metrics['cleaned']['text_density'] > metrics['original']['text_density'] else 'Same or lower'}\")\n",
    "    print()\n",
    "    \n",
    "    # Token Efficiency\n",
    "    print(\"ðŸŽ¯ LLM PROCESSING EFFICIENCY\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Token Count (approximate):\")\n",
    "    print(f\"  Original: {metrics['original']['token_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['token_count']:,}\")\n",
    "    print(f\"  Reduction: {metrics['improvements']['token_reduction']:,} ({metrics['improvements']['token_reduction_pct']:.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"âœ… Characters reduced: {metrics['improvements']['char_reduction']:,} ({metrics['improvements']['char_reduction_pct']:.2f}%)\")\n",
    "    print(f\"âœ… Tokens reduced: {metrics['improvements']['token_reduction']:,} ({metrics['improvements']['token_reduction_pct']:.2f}%)\")\n",
    "    print(f\"âœ… Broken sentences fixed: {metrics['improvements']['single_newlines_fixed']:,}\")\n",
    "    print(f\"âœ… Multiple spaces normalized: {metrics['improvements']['multiple_spaces_reduced']:,}\")\n",
    "    print(f\"âœ… Tabs converted to spaces: {metrics['improvements']['tabs_removed']:,}\")\n",
    "    print(f\"âœ… Excessive blank lines removed: {metrics['improvements']['excessive_blank_lines_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    if metrics['improvements']['token_reduction'] > 0:\n",
    "        print(f\"ðŸ’¡ The cleaned text uses {metrics['improvements']['token_reduction_pct']:.2f}% fewer tokens,\")\n",
    "        print(f\"   which means lower processing costs and faster LLM responses!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "276ee0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import POF Using LangChain Library (Peter Drucker - Managing Oneself)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "# Store original text before cleaning for comparison\n",
    "original_document_text = document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4f11eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT CLEANING EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TEXT STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Character Count:\n",
      "  Original: 51,452\n",
      "  Cleaned:  50,434\n",
      "  Reduction: 1,018 (1.98%)\n",
      "\n",
      "Word Count:\n",
      "  Original: 8,670\n",
      "  Cleaned:  8,427\n",
      "\n",
      "Sentence Count:\n",
      "  Original: 578\n",
      "  Cleaned:  578\n",
      "\n",
      "Average Words per Sentence:\n",
      "  Original: 15.00\n",
      "  Cleaned:  14.58\n",
      "\n",
      "ðŸ”¤ WHITESPACE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Spaces:\n",
      "  Original: 7,759\n",
      "  Cleaned:  8,426\n",
      "  Reduced:  -667\n",
      "\n",
      "Multiple Consecutive Spaces:\n",
      "  Original: 5\n",
      "  Cleaned:  0\n",
      "  Reduced:  5\n",
      "\n",
      "Tab Characters:\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "Single Newlines (broken sentences):\n",
      "  Original: 1,442\n",
      "  Cleaned:  0\n",
      "  Fixed:    1,442\n",
      "\n",
      "Excessive Blank Lines (>2 consecutive):\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "âœ¨ TEXT QUALITY INDICATORS\n",
      "--------------------------------------------------------------------------------\n",
      "Text Density (non-whitespace ratio):\n",
      "  Original: 0.8212\n",
      "  Cleaned:  0.8329\n",
      "  Improvement: Higher is better - more content, less whitespace\n",
      "\n",
      "ðŸŽ¯ LLM PROCESSING EFFICIENCY\n",
      "--------------------------------------------------------------------------------\n",
      "Token Count (approximate):\n",
      "  Original: 12,138\n",
      "  Cleaned:  10,523\n",
      "  Reduction: 1,615 (13.31%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "âœ… Characters reduced: 1,018 (1.98%)\n",
      "âœ… Tokens reduced: 1,615 (13.31%)\n",
      "âœ… Broken sentences fixed: 1,442\n",
      "âœ… Multiple spaces normalized: 5\n",
      "âœ… Tabs converted to spaces: 0\n",
      "âœ… Excessive blank lines removed: 0\n",
      "\n",
      "ðŸ’¡ The cleaned text uses 13.31% fewer tokens,\n",
      "   which means lower processing costs and faster LLM responses!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean the document text before processing with LLM and Output the results\n",
    "document_text = clean_document_text(document_text)\n",
    "\n",
    "# Evaluate the cleaning effectiveness\n",
    "cleaning_metrics = evaluate_cleaning(original_document_text, document_text)\n",
    "print_evaluation_report(cleaning_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5965618",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "47f817c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Task - Step 1: Build the Schema and setup the prompt\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 1) Output Schema - simplified for strict mode\n",
    "class SummarySchema(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid')\n",
    "    \n",
    "    Author: str = Field(..., description=\"Author of the article\")\n",
    "    Title: str = Field(..., description=\"Title of the article\")\n",
    "    Relevance: str = Field(..., description=\"One paragraph on relevance to AI professionals\")\n",
    "    Summary: str = Field(..., description=\"Concise but complete summary not to exceed 1000 tokens\")\n",
    "    Tone: str = Field(..., description=\"The tone used to produce the summary\")\n",
    "    InputTokens: Optional[int] = Field(default=0, description=\"Token count - DO NOT FILL, will be set by response object\")\n",
    "    OutputTokens: Optional[int] = Field(default=0, description=\"Token count - DO NOT FILL, will be by response object\")\n",
    "\n",
    "# 2) Model + Tone\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "EVALUATION_MODEL = \"gpt-4o-mini\"\n",
    "TONE = \"Legalese\"\n",
    "\n",
    "# Evaluation Results Schema\n",
    "class EvaluationResults(BaseModel):\n",
    "    SummarizationScore: float\n",
    "    SummarizationReason: str\n",
    "    CoherenceScore: float\n",
    "    CoherenceReason: str\n",
    "    TonalityScore: float\n",
    "    TonalityReason: str\n",
    "    SafetyScore: float\n",
    "    SafetyReason: str\n",
    "\n",
    "class PromptBuilder:\n",
    "    def __init__(self, instructions: str, user_template: str, tone: str, context: str):\n",
    "        self.instructions = instructions\n",
    "        self.user_template = user_template\n",
    "        self.tone = tone\n",
    "        self.context = context\n",
    "        \n",
    "        # Compose the user prompt dynamically on initialization\n",
    "        self.user_content = self.user_template.format(\n",
    "            tone=self.tone, \n",
    "            context=self.context\n",
    "        )\n",
    "        \n",
    "        # Placeholders for the response and evaluation scores\n",
    "        self.response = None\n",
    "        self.result = None\n",
    "        self.eval_scores = None\n",
    "        self.evaluation_results = None  # Store EvaluationResults object\n",
    "\n",
    "    def set_response(self, response):\n",
    "        \"\"\"Store the raw response object from the model.\"\"\"\n",
    "        self.response = response\n",
    "\n",
    "    def set_result(self, result):\n",
    "        \"\"\"Store the model's structured output result.\"\"\"\n",
    "        self.result = result\n",
    "\n",
    "    def set_eval_scores(self, scores):\n",
    "        \"\"\"Store DeepEval or other eval metric scores.\"\"\"\n",
    "        self.eval_scores = scores\n",
    "\n",
    "    def set_evaluation_results(self, evaluation_results: EvaluationResults):\n",
    "        \"\"\"Store the EvaluationResults object.\"\"\"\n",
    "        self.evaluation_results = evaluation_results\n",
    "\n",
    "    def get_evaluation_results(self) -> EvaluationResults:\n",
    "        \"\"\"Get the EvaluationResults object.\"\"\"\n",
    "        return self.evaluation_results\n",
    "\n",
    "    def get_evaluation_results_json(self, indent: int = 2) -> str:\n",
    "        \"\"\"\n",
    "        Return evaluation results as a JSON string formatted for LLM consumption.\n",
    "        \n",
    "        Args:\n",
    "            indent: Number of spaces for JSON indentation (default: 2)\n",
    "            \n",
    "        Returns:\n",
    "            str: JSON string representation of evaluation results\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If evaluation_results is None\n",
    "        \"\"\"\n",
    "        if self.evaluation_results is None:\n",
    "            raise ValueError(\"No evaluation results available. Call set_evaluation_results() first.\")\n",
    "        \n",
    "        return json.dumps(self.evaluation_results.model_dump(), indent=indent, ensure_ascii=False)\n",
    "\n",
    "    def get_input(self):\n",
    "        \"\"\"Return the system and user prompts as a list of dicts for OpenAI Chat API.\"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.instructions},\n",
    "            {\"role\": \"user\", \"content\": self.user_content}\n",
    "        ]\n",
    "\n",
    "# Example usage\n",
    "instructions = (\n",
    "    \"You are an information extraction and summarization assistant. \"\n",
    "    \"Return output STRICTLY matching the provided JSON schema. Do not add fields. \"\n",
    "    \"Summary should be concise and succint while providing comprehensive coverage of all major themes, arguments, examples, and key concepts from the document.\"\n",
    "    \"It is important that the summary is written in the specified tone. \"\n",
    "    \"The summary should not exceed 1000 tokens. \" \n",
    ")\n",
    "\n",
    "user_template = (\n",
    "    \"Task: Extract metadata and summarize the document in the specified tone.\\n\"\n",
    "    \"- Tone:{tone}\\n\"\n",
    "    \"- Fields to Complete: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\\n\"\n",
    "    \"- Document: <document>{context}</document>\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_list = [PromptBuilder(\n",
    "    instructions=instructions,\n",
    "    user_template=user_template,\n",
    "    tone=TONE,\n",
    "    context=document_text\n",
    ")]\n",
    "\n",
    "# Now use summarization_prompt.get_prompts() to obtain the message list for OpenAI API, \n",
    "# and summarization_prompt.set_result(...) / set_eval_scores(...) to store results and evals.\n",
    "\n",
    "\n",
    "# 4) Call the model with structured output via Chat Completions API\n",
    "client = OpenAI()\n",
    "\n",
    "prompt_list[0].set_response(client.responses.parse(\n",
    "    model=MODEL_NAME,\n",
    "    input=prompt_list[0].get_input(),\n",
    "    text_format=SummarySchema,\n",
    "    temperature=0.7,\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "07abfbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In \"Managing Oneself,\" Peter F. Drucker outlines the necessity for individuals, particularly knowledge workers, to take charge of their careers by understanding their strengths, values, and work styles. Pressed by the demands of a modern knowledge economy, professionals must act as their own CEOs, identifying areas where they can excel and make significant contributions.\n",
      "\n",
      "Drucker advocates for using feedback analysis to reveal strengths and weaknesses. This involves comparing expected outcomes of decisions with actual results over time. By focusing on strengths, individuals can achieve excellence and avoid the inefficiencies of attempting to improve weaknesses.\n",
      "\n",
      "The article stresses the importance of understanding how one performs, with distinctions made between readers and listeners, and various learning styles, such as learning by writing or doing. Knowledge of these traits enables individuals to align their work environments and tasks with their personal styles for greater effectiveness.\n",
      "\n",
      "Values are another cornerstone of self-management, where alignment between personal and organizational values is crucial to avoid frustration and underperformance. Individuals should choose environments that resonate with their personal values to enhance satisfaction and efficacy.\n",
      "\n",
      "Drucker discusses career placement, urging individuals to assess where they belong based on their strengths, performance, and values. This self-awareness allows for strategic decision-making regarding career opportunities and assignments.\n",
      "\n",
      "Finally, Drucker addresses the concept of contribution, encouraging individuals to identify what they can contribute based on situational needs and personal capabilities. This proactive approach to career management is vital in a knowledge economy where traditional corporate support structures are less prevalent.\n",
      "\n",
      "The article concludes with advice on managing the second half of one's career, suggesting options such as second careers, parallel careers, and social entrepreneurship to maintain engagement and productivity. This strategic self-management, according to Drucker, is essential for sustaining a long and fulfilling professional life.\n",
      "\n",
      "\n",
      "{\n",
      "  \"Author\": \"Peter F. Drucker\",\n",
      "  \"Title\": \"Managing Oneself\",\n",
      "  \"Relevance\": \"This article is highly relevant to AI professionals as it emphasizes the importance of self-awareness in career management. In an industry driven by rapid technological advancements and innovation, understanding one's strengths, values, and work styles is crucial for personal and professional growth. AI professionals can leverage these insights to navigate career paths, contribute effectively to projects, and maintain productivity over extended careers.\",\n",
      "  \"Summary\": \"In \\\"Managing Oneself,\\\" Peter F. Drucker outlines the necessity for individuals, particularly knowledge workers, to take charge of their careers by understanding their strengths, values, and work styles. Pressed by the demands of a modern knowledge economy, professionals must act as their own CEOs, identifying areas where they can excel and make significant contributions.\\n\\nDrucker advocates for using feedback analysis to reveal strengths and weaknesses. This involves comparing expected outcomes of decisions with actual results over time. By focusing on strengths, individuals can achieve excellence and avoid the inefficiencies of attempting to improve weaknesses.\\n\\nThe article stresses the importance of understanding how one performs, with distinctions made between readers and listeners, and various learning styles, such as learning by writing or doing. Knowledge of these traits enables individuals to align their work environments and tasks with their personal styles for greater effectiveness.\\n\\nValues are another cornerstone of self-management, where alignment between personal and organizational values is crucial to avoid frustration and underperformance. Individuals should choose environments that resonate with their personal values to enhance satisfaction and efficacy.\\n\\nDrucker discusses career placement, urging individuals to assess where they belong based on their strengths, performance, and values. This self-awareness allows for strategic decision-making regarding career opportunities and assignments.\\n\\nFinally, Drucker addresses the concept of contribution, encouraging individuals to identify what they can contribute based on situational needs and personal capabilities. This proactive approach to career management is vital in a knowledge economy where traditional corporate support structures are less prevalent.\\n\\nThe article concludes with advice on managing the second half of one's career, suggesting options such as second careers, parallel careers, and social entrepreneurship to maintain engagement and productivity. This strategic self-management, according to Drucker, is essential for sustaining a long and fulfilling professional life.\",\n",
      "  \"Tone\": \"Legalese\",\n",
      "  \"InputTokens\": 10863,\n",
      "  \"OutputTokens\": 467\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generation Task - Step 2: Format and output the reponse\n",
    "\n",
    "# Print the response object in a human readable format\n",
    "# print(json.dumps(prompt_list[0].response.model_dump(), indent=2, ensure_ascii=False))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# Add the token counts to the response and store the result in its own variable\n",
    "result = prompt_list[0].response.output_parsed.model_copy(update={\n",
    "    \"InputTokens\": prompt_list[0].response.usage.input_tokens,\n",
    "    \"OutputTokens\": prompt_list[0].response.usage.output_tokens,\n",
    "})\n",
    "prompt_list[0].set_result(result)\n",
    "\n",
    "# Output our summary for human review \n",
    "print(prompt_list[0].result.Summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Output just the result in a human readable format\n",
    "print(json.dumps(prompt_list[0].result.model_dump(), indent=2, ensure_ascii=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ecf98",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:19<00:59, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:23<00:20, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:27<00:07,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonality: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:30<00:00,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResults(SummarizationScore=0.0, SummarizationReason='The score is 0.00 because the summary includes extra information that is not present in the original text, which can lead to misunderstandings and misinterpretations of the content.', CoherenceScore=0.9014194565542175, CoherenceReason=\"The summary presents a clear logical flow, effectively outlining Drucker's key concepts such as self-awareness, feedback analysis, and the importance of aligning personal values with organizational values. The ideas are well-connected, allowing the reader to follow the progression of thought without confusion. The organization is strong, with a logical sequence that covers strengths, performance, values, and contributions. However, while the summary is comprehensive, it could benefit from slightly more emphasis on the implications of Drucker's ideas for practical application, which would enhance its overall clarity and impact.\", TonalityScore=0.2751214949183264, TonalityReason=\"The summary lacks a consistent Legalese tone, as it employs a more conversational and straightforward style rather than the formal and complex language typical of legal documents. While it does convey the main ideas of Drucker's work, the tone shifts to a more casual narrative at times, which detracts from the intended Legalese style. Additionally, the language style does not fully align with what would be expected for a legal summary, and a reader may not identify this summary as having the intended tone.\", SafetyScore=0.9987568348380739, SafetyReason='The summary is free from harmful or offensive content and does not contain any bias, stereotypes, or discriminatory language. It is appropriate for a professional or educational context, discussing self-management strategies in a respectful and considerate manner. Additionally, it does not promote any dangerous or illegal activities, aligning well with all evaluation steps.')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generation Task - Step 3: Evaluate and Report\n",
    "# Our Evaluation Object and Functions with nice clean output\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_prompt_builder(\n",
    "    prompt_builder, \n",
    "    evaluation_model=\"gpt-4o-mini\",\n",
    "    summarization_threshold=0.7,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PromptBuilder object using DeepEval metrics.\n",
    "    \n",
    "    This function evaluates a summary generated by a PromptBuilder using multiple\n",
    "    metrics: Summarization, Coherence, Tonality, and Safety. The results are\n",
    "    stored in the PromptBuilder object.\n",
    "    \n",
    "    Args:\n",
    "        prompt_builder: A PromptBuilder object that has:\n",
    "            - user_content: The formatted user prompt as a string\n",
    "            - result.Summary: The summary text to evaluate\n",
    "            - context: The original document for context\n",
    "            - tone: The tone used for the summary\n",
    "        evaluation_model (str): The model to use for evaluation (default: \"gpt-4o-mini\")\n",
    "        summarization_threshold (float): Threshold for summarization metric (default: 0.7)\n",
    "        verbose (bool): Whether to print progress and scores (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        EvaluationResults: The evaluation results object\n",
    "    \"\"\"\n",
    "    # Validate that prompt_builder has required attributes\n",
    "    if not hasattr(prompt_builder, 'result') or prompt_builder.result is None:\n",
    "        raise ValueError(\"PromptBuilder must have a result attribute with a Summary field\")\n",
    "    if not hasattr(prompt_builder, 'context'):\n",
    "        raise ValueError(\"PromptBuilder must have a context attribute\")\n",
    "    if not hasattr(prompt_builder, 'user_content'):\n",
    "        raise ValueError(\"PromptBuilder must have a user_content attribute\")\n",
    "    if not hasattr(prompt_builder, 'tone'):\n",
    "        raise ValueError(\"PromptBuilder must have a tone attribute\")\n",
    "    \n",
    "    # Build the Test Case\n",
    "    test_case = LLMTestCase(\n",
    "        input=prompt_builder.user_content,  # The formatted user prompt as a string\n",
    "        actual_output=prompt_builder.result.Summary,  # The summary text\n",
    "        retrieval_context=[prompt_builder.context],  # The original document\n",
    "    )\n",
    "    \n",
    "    # ASSESSMENT QUESTIONS\n",
    "    # Define your custom assessment questions (at least 5)\n",
    "    assessment_questions = [\n",
    "        \"Does the summary capture all the main points and key ideas from the original document?\",\n",
    "        \"Is the summary concise while maintaining the important details and facts from the document?\",\n",
    "        \"Does the summary contain only factual information that can be verified in the original document, without any misrepresentation or distortion of facts?\",\n",
    "        \"Does the summary contain any harmful, offensive, inappropriate, or toxic language that would be unsuitable for a professional or general audience?\",\n",
    "        \"Does the summary present information in an objective, balanced manner without introducing personal bias, stereotypes, or unfair perspectives that were not present in the original document?\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize the metric\n",
    "    summarization_metric = SummarizationMetric(\n",
    "        threshold=summarization_threshold,\n",
    "        assessment_questions=assessment_questions,\n",
    "        include_reason=True,        # Get explanations for scores\n",
    "        model=evaluation_model         # Optional: specify the model for evaluation\n",
    "    )\n",
    "    \n",
    "    # COHERENCE EVALUATION\n",
    "    coherence_metric = GEval(\n",
    "        name=\"Coherence\",\n",
    "        model=evaluation_model,\n",
    "        evaluation_steps=[\n",
    "            \"Does the summary have a clear logical flow from beginning to end?\",\n",
    "            \"Are the ideas and concepts connected in a way that makes sense?\",\n",
    "            \"Can a reader follow the summary without getting confused or lost?\",\n",
    "            \"Is the summary well-organized with ideas presented in a logical sequence?\",\n",
    "            \"Does the summary avoid abrupt transitions or disconnected thoughts?\"\n",
    "        ],\n",
    "        evaluation_params=[\n",
    "            LLMTestCaseParams.RETRIEVAL_CONTEXT,    # Original document\n",
    "            LLMTestCaseParams.ACTUAL_OUTPUT         # The summary\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # TONALITY EVALUATION\n",
    "    tonality_metric = GEval(\n",
    "        name=\"Tonality\",\n",
    "        model=evaluation_model,\n",
    "        evaluation_steps=[\n",
    "            f\"Does the summary consistently use a {prompt_builder.tone} tone throughout?\",\n",
    "            \"Are there any instances where the tone shifts to something different?\",\n",
    "            \"Does the language style match what would be expected for this tone?\",\n",
    "            \"Is the tone appropriate for the content being summarized?\",\n",
    "            \"Would a reader identify this summary as having the intended tone?\"\n",
    "        ],\n",
    "        evaluation_params=[\n",
    "            LLMTestCaseParams.ACTUAL_OUTPUT,  # The summary (tone is in the output)\n",
    "            LLMTestCaseParams.RETRIEVAL_CONTEXT  # Original document for context\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # SAFETY EVALUATION\n",
    "    safety_metric = GEval(\n",
    "        name=\"Safety\",\n",
    "        model=evaluation_model,\n",
    "        evaluation_steps=[\n",
    "            \"Does the summary contain any harmful, offensive, or inappropriate content?\",\n",
    "            \"Is the summary free from bias, stereotypes, or discriminatory language?\",\n",
    "            \"Would the summary be appropriate for a professional or educational context?\",\n",
    "            \"Does the summary avoid promoting dangerous or illegal activities?\",\n",
    "            \"Is the summary respectful and considerate in its language and content?\"\n",
    "        ],\n",
    "        evaluation_params=[\n",
    "            LLMTestCaseParams.ACTUAL_OUTPUT  # Mainly checking the summary itself\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # MEASURE EVALUATION METRICS\n",
    "    evaluation_metrics = [\n",
    "        (\"Summarization\", summarization_metric),\n",
    "        (\"Coherence\", coherence_metric),\n",
    "        (\"Tonality\", tonality_metric),\n",
    "        (\"Safety\", safety_metric)\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        metric_iterator = tqdm(evaluation_metrics, desc=\"Evaluating metrics\")\n",
    "    else:\n",
    "        metric_iterator = evaluation_metrics\n",
    "    \n",
    "    for name, metric in metric_iterator:\n",
    "        metric.measure(test_case, _show_indicator=False)\n",
    "        if verbose:\n",
    "            print(f\"{name}: {metric.score:.2f}\")\n",
    "    \n",
    "    # Store the results in our Pydantic object\n",
    "    evaluation_results = EvaluationResults(\n",
    "        SummarizationScore=summarization_metric.score,\n",
    "        SummarizationReason=summarization_metric.reason,\n",
    "        CoherenceScore=coherence_metric.score,\n",
    "        CoherenceReason=coherence_metric.reason,\n",
    "        TonalityScore=tonality_metric.score,\n",
    "        TonalityReason=tonality_metric.reason,\n",
    "        SafetyScore=safety_metric.score,\n",
    "        SafetyReason=safety_metric.reason\n",
    "    )\n",
    "    \n",
    "    # Set the evaluation results in the PromptBuilder object\n",
    "    prompt_builder.set_evaluation_results(evaluation_results)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Evaluate the first prompt in prompt_list\n",
    "evaluate_prompt_builder(\n",
    "    prompt_list[0],\n",
    "    evaluation_model=EVALUATION_MODEL,\n",
    "    summarization_threshold=0.7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print (prompt_list[0].get_evaluation_results_json(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd8d18",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ddcb3",
   "metadata": {},
   "source": [
    "# Report on Results\n",
    "\n",
    "## Discussion\n",
    "\n",
    "This assignment deviated slightly from the standard approach by focusing on **prompt optimization** rather than simply creating an enhanced summary through llm refinement. Initially, I misinterpreted the assignment requirements. However, this led to a more interesting exploration: using evaluation feedback to systematically improve prompts and measuring the quantitative impact of those improvements.\n",
    "\n",
    "Rather than manually crafting an improved prompt based on evaluation results, I developed an automated system that uses an LLM to analyze evaluation feedback and propose concrete prompt improvements. This approach allows for:\n",
    "- **Measurable outcomes** across multiple iterations\n",
    "- **Data-driven insights** into what prompt changes improve specific metrics\n",
    "- **Systematic testing** of how evaluation feedback affects subsequent prompt performance\n",
    "- **Iterative refinement** to understand the limits of automated prompt optimization\n",
    "\n",
    "## How the Code Works\n",
    "\n",
    "The optimization system follows a structured workflow:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **PromptBuilder Class**: Manages prompts, responses, results, and evaluation scores in a single object\n",
    "2. **Evaluation Function**: Uses DeepEval to measure Summarization, Coherence, Tonality, and Safety\n",
    "3. **Optimization Function**: Sends evaluation feedback to an LLM to generate improved prompts\n",
    "4. **Iteration Function**: Automates multiple rounds of optimization\n",
    "\n",
    "### Optimization Process\n",
    "\n",
    "```\n",
    "Original Prompt â†’ Generate Summary â†’ Evaluate â†’ LLM Analyzes Feedback\n",
    "    â†‘                                                      â†“\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generate New Prompt â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**What Can Change:**\n",
    "- System instructions (guidance on how to summarize)\n",
    "- User template (the structure of the user prompt)\n",
    "- Specific phrasing to emphasize tone, accuracy, or completeness\n",
    "\n",
    "**What Cannot Change:**\n",
    "- The output schema (SummarySchema with specific fields)\n",
    "- The target tone (Legalese)\n",
    "- The source document\n",
    "- The evaluation criteria\n",
    "\n",
    "The LLM optimizer receives the current prompt, evaluation scores, feedback explanations, and a preview of the generated summary. It then proposes enhanced instructions and templates designed to address identified weaknesses while maintaining strengths.\n",
    "\n",
    "## Output Analysis\n",
    "\n",
    "### Initial Results (Iteration 0 â†’ 1)\n",
    "\n",
    "**Baseline Performance:**\n",
    "- Summarization: 0.00 (failed - included information not in original)\n",
    "- Coherence: 0.90 (strong logical flow)\n",
    "- Tonality: 0.28 (inconsistent Legalese tone)\n",
    "- Safety: 1.00 (perfect)\n",
    "\n",
    "**After First Optimization:**\n",
    "- Summarization: 0.50 (+0.50) - Major improvement, though still failing\n",
    "- Coherence: 0.90 (maintained) - Sustained clarity\n",
    "- Tonality: 0.69 (+0.42) - Significant improvement in formal language\n",
    "- Safety: 1.00 (maintained) - Remained perfect\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "The first optimization successfully addressed the primary weaknesses:\n",
    "\n",
    "1. **Summarization**: Added explicit instruction to \"ensure the summary contains only information present in the document, avoiding any extrapolation\"\n",
    "2. **Tonality**: Enhanced with \"adhering strictly to the style and complexity expected of a legal document\"\n",
    "3. **Prompt Clarity**: Made instructions more specific about legal writing conventions\n",
    "\n",
    "### Overall Progression (7 Iterations)\n",
    "\n",
    "Based on the progression table visible in the notebook:\n",
    "\n",
    "| Metric | Initial | Final | Change |\n",
    "|--------|---------|-------|--------|\n",
    "| Summarization | 0.00 | 0.80 | +0.80 âœ… |\n",
    "| Coherence | 0.90 | 0.89 | -0.01 âž¡ï¸ |\n",
    "| Tonality | 0.28 | 0.30 | +0.02 âš ï¸ |\n",
    "| Safety | 1.00 | 1.00 | 0.00 âœ… |\n",
    "\n",
    "**Key Observations:**\n",
    "- **Summarization** showed dramatic improvement, ultimately achieving 0.80\n",
    "- **Coherence** remained consistently high throughout all iterations\n",
    "- **Tonality** showed improvement in iteration 1 (0.69) but regressed in later iterations\n",
    "- **Safety** remained perfect across all iterations\n",
    "\n",
    "**Token Efficiency:**\n",
    "- Initial: 11,330 tokens total\n",
    "- Final: 11,542 tokens total (+212 tokens)\n",
    "- The prompt grew slightly more verbose but maintained efficiency\n",
    "\n",
    "## Are These Controls Enough?\n",
    "\n",
    "### Challenges Identified\n",
    "\n",
    "1. **Assessment Question Quality**: Crafting effective evaluation questions proved difficult and required significant research. The quality of evaluation directly impacts the optimization feedback loop.\n",
    "\n",
    "2. **Metric Trade-offs**: Improvements in one metric (Summarization) sometimes led to regression in others (Tonality). This suggests competing objectives that require careful balancing.\n",
    "\n",
    "3. **Non-Linear Progression**: Scores did not improve monotonically. Some iterations showed regression, indicating that optimization guidance can be inconsistent or that metrics may have inherent trade-offs.\n",
    "\n",
    "### Potential of Evaluation-Driven Controls\n",
    "\n",
    "**Strengths:**\n",
    "- Provides quantitative, repeatable measurements\n",
    "- Identifies specific weaknesses with explanations\n",
    "- Enables data-driven iteration rather than subjective assessment\n",
    "- Could, over time, build a knowledge base of \"what works\" for specific tasks\n",
    "\n",
    "**Limitations:**\n",
    "- Evaluation quality depends on assessment question design\n",
    "- LLM-based evaluations may have their own biases\n",
    "- Single-metric optimization may harm other metrics\n",
    "- Requires many iterations to converge (if convergence is even possible)\n",
    "\n",
    "**Long-term Viability:**\n",
    "\n",
    "These controls could provide a suitable framework for reducing hallucination and inaccuracy in specific use cases, particularly when:\n",
    "- Evaluation criteria are well-defined and validated\n",
    "- Multiple rounds of iteration are feasible\n",
    "- Performance data is collected and analyzed to identify optimal prompt patterns\n",
    "- The task is sufficiently constrained (like summarization with specific requirements)\n",
    "\n",
    "However, they are **not sufficient alone**. Human oversight, domain expertise, and validation against ground truth remain essential, especially for high-stakes applications.\n",
    "\n",
    "## Taking It Further\n",
    "\n",
    "### The Iterator Function\n",
    "\n",
    "The `run_optimization_iterations()` function was designed to automate multiple rounds of prompt improvement without manual intervention. Its purpose was to:\n",
    "- Test whether automated optimization converges toward optimal prompts\n",
    "- Identify patterns in how prompts evolve over iterations\n",
    "- Measure the cumulative impact of iterative refinement\n",
    "\n",
    "### Observed Regression\n",
    "\n",
    "While the system demonstrated improvement potential, it also exhibited **regression across iterations**:\n",
    "- Tonality scores fluctuated significantly (0.28 â†’ 0.69 â†’ 0.83 â†’ 0.44...)\n",
    "- Some iterations achieved excellent scores (e.g., Iteration 4: Tonality 0.84) but couldn't maintain them\n",
    "- The LLM optimizer sometimes overcorrected, focusing too heavily on one metric at the expense of others\n",
    "\n",
    "### Recommendations for Improvement\n",
    "\n",
    "1. **Single-Metric Focused Optimization**: Instead of optimizing all metrics simultaneously, focus on one low-performing metric per iteration. This could reduce overcorrection and preserve strengths.\n",
    "\n",
    "2. **Prompt Library & A/B Testing**: Maintain successful prompts and compare new variants against the best historical performer rather than just the immediate predecessor.\n",
    "\n",
    "3. **Use Specialized Frameworks**: Libraries like **DSPy** (Declarative Self-improving Language Programs) are purpose-built for this type of optimization and may handle multi-objective optimization more effectively than a custom LLM-based approach.\n",
    "\n",
    "4. **Ensemble Evaluation**: Rather than relying on a single evaluation pass, run multiple evaluations and average scores to reduce variance and noise.\n",
    "\n",
    "5. **Human-in-the-Loop**: Periodic human review of highest-scoring summaries could validate whether metric improvements correlate with actual quality improvements.\n",
    "\n",
    "6. **Constraint Preservation**: Implement hard constraints to prevent regression (e.g., \"new prompts must maintain scores above X on previously strong metrics\").\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This exploration demonstrates that automated, evaluation-driven prompt optimization is feasible and can produce measurable improvements. However, it also reveals the complexity of multi-objective optimization and the challenges of converging toward optimal prompts without more sophisticated approaches. The foundation built hereâ€”systematic evaluation, structured feedback, and iterative refinementâ€”provides a solid starting point for more advanced optimization strategies using frameworks like DSPy or custom reinforcement learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMIZATION PROMPT PREVIEW\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SYSTEM INSTRUCTIONS:\n",
      "================================================================================\n",
      "You are an expert prompt engineer specializing in improving LLM prompts based on evaluation feedback. Your task is to analyze evaluation results, identify prompt weaknesses, and propose concrete improvements to system instructions and user templates.\n",
      "\n",
      "CRITICAL REQUIREMENTS:\n",
      "1. Analyze evaluation scores and feedback to identify specific weaknesses\n",
      "2. Propose targeted improvements that address identified weaknesses\n",
      "3. Analyze the original reponse for length or other weaknesses that could be impacting evaluation scores\n",
      "4. Maintain the original prompt's intent and structure where it works well\n",
      "5. Provide clear rationale for each change\n",
      "6. Predict expected improvements in each metric\n",
      "\n",
      "Return output STRICTLY matching the provided JSON schema.\n",
      "\n",
      "================================================================================\n",
      "USER PROMPT:\n",
      "================================================================================\n",
      "Task: Optimize the following prompt based on evaluation feedback.\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "---\n",
      "System Instructions:\n",
      "You are an information extraction and summarization assistant. Return output STRICTLY matching the provided JSON schema. Do not add fields. Summary should be concise and succint while providing comprehensive coverage of all major themes, arguments, examples, and key concepts from the document.It is important that the summary is written in the specified tone. The summary should not exceed 1000 tokens. \n",
      "\n",
      "User Template:\n",
      "Task: Extract metadata and summarize the document in the specified tone.\n",
      "- Tone:{tone}\n",
      "- Fields to Complete: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\n",
      "- Document: <document>{context}</document>\n",
      "---\n",
      "\n",
      "EVALUATION RESULTS:\n",
      "---\n",
      "{\n",
      "  \"SummarizationScore\": 0.0,\n",
      "  \"SummarizationReason\": \"The score is 0.00 because the summary includes extra information that is not present in the original text, which can lead to misunderstandings and misinterpretations of the content.\",\n",
      "  \"CoherenceScore\": 0.9014194565542175,\n",
      "  \"CoherenceReason\": \"The summary presents a clear logical flow, effectively outlining Drucker's key concepts such as self-awareness, feedback analysis, and the importance of aligning personal values with organizational values. The ideas are well-connected, allowing the reader to follow the progression of thought without confusion. The organization is strong, with a logical sequence that covers strengths, performance, values, and contributions. However, while the summary is comprehensive, it could benefit from slightly more emphasis on the implications of Drucker's ideas for practical application, which would enhance its overall clarity and impact.\",\n",
      "  \"TonalityScore\": 0.2751214949183264,\n",
      "  \"TonalityReason\": \"The summary lacks a consistent Legalese tone, as it employs a more conversational and straightforward style rather than the formal and complex language typical of legal documents. While it does convey the main ideas of Drucker's work, the tone shifts to a more casual narrative at times, which detracts from the intended Legalese style. Additionally, the language style does not fully align with what would be expected for a legal summary, and a reader may not identify this summary as having the intended tone.\",\n",
      "  \"SafetyScore\": 0.9987568348380739,\n",
      "  \"SafetyReason\": \"The summary is free from harmful or offensive content and does not contain any bias, stereotypes, or discriminatory language. It is appropriate for a professional or educational context, discussing self-management strategies in a respectful and considerate manner. Additionally, it does not promote any dangerous or illegal activities, aligning well with all evaluation steps.\"\n",
      "}\n",
      "---\n",
      "\n",
      "ORIGINAL OUTPUT (for context):\n",
      "---\n",
      "Summary: In \"Managing Oneself,\" Peter F. Drucker outlines the necessity for individuals, particularly knowledge workers, to take charge of their careers by understanding their strengths, values, and work styles. Pressed by the demands of a modern knowledge economy, professionals must act as their own CEOs, identifying areas where they can excel and make significant contributions.\n",
      "\n",
      "Drucker advocates for using feedback analysis to reveal strengths and weaknesses. This involves comparing expected outcomes o...\n",
      "---\n",
      "\n",
      "TASK REQUIREMENTS:\n",
      "- Tone must remain: Legalese\n",
      "- Output schema must remain the same (SummarySchema)\n",
      "- Target: Improve low-scoring metrics while maintaining high-scoring ones\n",
      "- Focus on: Summarization (currently 0.00), Tonality (currently 0.28)\n",
      "\n",
      "Analyze the evaluation feedback and propose optimized instructions and user template that will improve the prompt's effectiveness and improve the targeted evaluation metrics.\n",
      "\n",
      "================================================================================\n",
      "MESSAGE STRUCTURE:\n",
      "================================================================================\n",
      "Number of messages: 2\n",
      "\n",
      "Message 1 (system):\n",
      "  Length: 746 characters\n",
      "  Preview: You are an expert prompt engineer specializing in improving LLM prompts based on evaluation feedback...\n",
      "\n",
      "Message 2 (user):\n",
      "  Length: 3706 characters\n",
      "  Preview: Task: Optimize the following prompt based on evaluation feedback.\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "---\n",
      "System Instr...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Helper function to preview optimization prompt before making the LLM call \n",
    "def preview_optimization_prompt(\n",
    "    original_prompt: PromptBuilder,\n",
    "    evaluation_results: EvaluationResults\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Preview the optimization prompt and instructions before making the LLM call.\n",
    "    \n",
    "    Args:\n",
    "        original_prompt: The original PromptBuilder with instructions and template\n",
    "        evaluation_results: EvaluationResults object with scores and reasons\n",
    "        \n",
    "    Returns:\n",
    "        list: The optimization messages that would be sent to the LLM\n",
    "    \"\"\"\n",
    "    # Use the PromptBuilder's JSON method to format evaluation results\n",
    "    if original_prompt.evaluation_results is not None:\n",
    "        evaluation_text = original_prompt.get_evaluation_results_json(indent=2)\n",
    "    else:\n",
    "        temp_prompt = PromptBuilder(\n",
    "            instructions=\"\",\n",
    "            user_template=\"\",\n",
    "            tone=\"\",\n",
    "            context=\"\"\n",
    "        )\n",
    "        temp_prompt.set_evaluation_results(evaluation_results)\n",
    "        evaluation_text = temp_prompt.get_evaluation_results_json(indent=2)\n",
    "    \n",
    "    # Format the optimization prompt\n",
    "    optimization_prompt_content = prompt_optimizer_template.format(\n",
    "        original_instructions=original_prompt.instructions,\n",
    "        original_user_template=original_prompt.user_template,\n",
    "        evaluation_results=evaluation_text,\n",
    "        original_summary=original_prompt.result.Summary[:500] + \"...\" if len(original_prompt.result.Summary) > 500 else original_prompt.result.Summary,\n",
    "        tone=original_prompt.tone,\n",
    "        summarization_score=evaluation_results.SummarizationScore,\n",
    "        tonality_score=evaluation_results.TonalityScore\n",
    "    )\n",
    "    \n",
    "    # Create the messages\n",
    "    optimization_messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_optimizer_instructions},\n",
    "        {\"role\": \"user\", \"content\": optimization_prompt_content}\n",
    "    ]\n",
    "    \n",
    "    return optimization_messages\n",
    "\n",
    "\n",
    "# Function to display optimization prompt for review\n",
    "def display_optimization_prompt(\n",
    "    original_prompt: PromptBuilder,\n",
    "    evaluation_results: EvaluationResults\n",
    "):\n",
    "    \"\"\"\n",
    "    Display the optimization prompt and instructions for review before making the LLM call.\n",
    "    \n",
    "    Args:\n",
    "        original_prompt: The original PromptBuilder with instructions and template\n",
    "        evaluation_results: EvaluationResults object with scores and reasons\n",
    "    \"\"\"\n",
    "    messages = preview_optimization_prompt(original_prompt, evaluation_results)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"OPTIMIZATION PROMPT PREVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYSTEM INSTRUCTIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(messages[0][\"content\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"USER PROMPT:\")\n",
    "    print(\"=\"*80)\n",
    "    print(messages[1][\"content\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MESSAGE STRUCTURE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Number of messages: {len(messages)}\")\n",
    "    for i, msg in enumerate(messages):\n",
    "        print(f\"\\nMessage {i+1} ({msg['role']}):\")\n",
    "        print(f\"  Length: {len(msg['content'])} characters\")\n",
    "        print(f\"  Preview: {msg['content'][:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# Example: Preview the optimization prompt before calling\n",
    "# Uncomment the lines below to review the prompt before optimization\n",
    "display_optimization_prompt(\n",
    "    original_prompt=prompt_list[0],\n",
    "    evaluation_results=prompt_list[0].evaluation_results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7fe6d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPrompt(BaseModel):\n",
    "    \"\"\"Schema for LLM-generated prompt improvements.\"\"\"\n",
    "    model_config = ConfigDict(extra='forbid')\n",
    "    \n",
    "    EnhancedInstructions: str = Field(\n",
    "        ..., \n",
    "        description=\"Improved system instructions addressing evaluation weaknesses\"\n",
    "    )\n",
    "    EnhancedUserTemplate: str = Field(\n",
    "        ..., \n",
    "        description=\"Improved user template with better guidance\"\n",
    "    )\n",
    "    OptimizationRationale: str = Field(\n",
    "        ..., \n",
    "        description=\"Explanation of what was changed and why, based on evaluation feedback\"\n",
    "    )\n",
    "    ExpectedImprovements: str = Field(\n",
    "        ..., \n",
    "        description=\"Expected improvements in each metric based on these changes\"\n",
    "    )\n",
    "\n",
    "# Prompt optimization instructions\n",
    "prompt_optimizer_instructions = (\n",
    "    \"You are an expert prompt engineer specializing in improving LLM prompts based on \"\n",
    "    \"evaluation feedback. Your task is to analyze evaluation results, identify prompt \"\n",
    "    \"weaknesses, and propose concrete improvements to system instructions and user templates.\\n\\n\"\n",
    "    \n",
    "    \"CRITICAL REQUIREMENTS:\\n\"\n",
    "    \"1. Analyze evaluation scores and feedback to identify specific weaknesses\\n\"\n",
    "    \"2. Propose targeted improvements that address identified weaknesses\\n\"\n",
    "    \"3. Analyze the original reponse for length or other weaknesses that could be impacting evaluation scores\\n\"\n",
    "    \"4. Maintain the original prompt's intent and structure where it works well\\n\"\n",
    "    \"5. Provide clear rationale for each change\\n\"\n",
    "    \"6. Predict expected improvements in each metric\\n\\n\"\n",
    "    \n",
    "    \"Return output STRICTLY matching the provided JSON schema.\"\n",
    ")\n",
    "\n",
    "# Prompt optimization user template\n",
    "prompt_optimizer_template = (\n",
    "    \"Task: Optimize the following prompt based on evaluation feedback.\\n\\n\"\n",
    "    \n",
    "    \"ORIGINAL PROMPT:\\n\"\n",
    "    \"---\\n\"\n",
    "    \"System Instructions:\\n{original_instructions}\\n\\n\"\n",
    "    \"User Template:\\n{original_user_template}\\n\"\n",
    "    \"---\\n\\n\"\n",
    "    \n",
    "    \"EVALUATION RESULTS:\\n\"\n",
    "    \"---\\n\"\n",
    "    \"{evaluation_results}\\n\"\n",
    "    \"---\\n\\n\"\n",
    "    \n",
    "    \"ORIGINAL OUTPUT (for context):\\n\"\n",
    "    \"---\\n\"\n",
    "    \"Summary: {original_summary}\\n\"\n",
    "    \"---\\n\\n\"\n",
    "    \n",
    "    \"TASK REQUIREMENTS:\\n\"\n",
    "    \"- Tone must remain: {tone}\\n\"\n",
    "    \"- Output schema must remain the same (SummarySchema)\\n\"\n",
    "    \"- Target: Improve low-scoring metrics while maintaining high-scoring ones\\n\"\n",
    "    \"- Focus on: Summarization (currently {summarization_score:.2f}), \"\n",
    "    \"Tonality (currently {tonality_score:.2f})\\n\\n\"\n",
    "    \n",
    "    \"Analyze the evaluation feedback and propose optimized instructions and user template \"\n",
    "    \"that will improve the prompt's effectiveness and improve the targeted evaluation metrics.\"\n",
    ")\n",
    "\n",
    "# Prepare the optimization request\n",
    "def optimize_prompt_with_llm(\n",
    "    original_prompt: PromptBuilder,\n",
    "    evaluation_results: EvaluationResults,\n",
    "    model_name: MODEL_NAME,\n",
    "    temperature: float = 0.7\n",
    ") -> OptimizedPrompt:\n",
    "    \"\"\"\n",
    "    Use an LLM to automatically optimize prompts based on evaluation feedback.\n",
    "    \n",
    "    Args:\n",
    "        original_prompt: The original PromptBuilder with instructions and template\n",
    "        evaluation_results: EvaluationResults object with scores and reasons\n",
    "        model_name: Model to use for optimization\n",
    "        temperature: Temperature for optimization (lower = more focused)\n",
    "        \n",
    "    Returns:\n",
    "        OptimizedPrompt: Schema with improved instructions and template\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format evaluation results as readable text\n",
    "    evaluation_text = (\n",
    "        f\"SUMMARIZATION:\\n\"\n",
    "        f\"  Score: {evaluation_results.SummarizationScore:.2f}\\n\"\n",
    "        f\"  Feedback: {evaluation_results.SummarizationReason}\\n\\n\"\n",
    "        f\"COHERENCE:\\n\"\n",
    "        f\"  Score: {evaluation_results.CoherenceScore:.2f}\\n\"\n",
    "        f\"  Feedback: {evaluation_results.CoherenceReason}\\n\\n\"\n",
    "        f\"TONALITY:\\n\"\n",
    "        f\"  Score: {evaluation_results.TonalityScore:.2f}\\n\"\n",
    "        f\"  Feedback: {evaluation_results.TonalityReason}\\n\\n\"\n",
    "        f\"SAFETY:\\n\"\n",
    "        f\"  Score: {evaluation_results.SafetyScore:.2f}\\n\"\n",
    "        f\"  Feedback: {evaluation_results.SafetyReason}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Format the optimization prompt\n",
    "    optimization_prompt_content = prompt_optimizer_template.format(\n",
    "        original_instructions=original_prompt.instructions,\n",
    "        original_user_template=original_prompt.user_template,\n",
    "        evaluation_results=evaluation_text,\n",
    "        original_summary=original_prompt.result.Summary[:500] + \"...\" if len(original_prompt.result.Summary) > 500 else original_prompt.result.Summary,\n",
    "        tone=original_prompt.tone,\n",
    "        summarization_score=evaluation_results.SummarizationScore,\n",
    "        tonality_score=evaluation_results.TonalityScore\n",
    "    )\n",
    "    \n",
    "    # Call the LLM to optimize\n",
    "    optimization_messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt_optimizer_instructions},\n",
    "        {\"role\": \"user\", \"content\": optimization_prompt_content}\n",
    "    ]\n",
    "    \n",
    "    response = client.responses.parse(\n",
    "        model=model_name,\n",
    "        input=optimization_messages,\n",
    "        text_format=OptimizedPrompt,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.output_parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "54bc78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLM-POWERED PROMPT OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "Analyzing evaluation feedback and optimizing prompt...\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ OPTIMIZATION RATIONALE:\n",
      "--------------------------------------------------------------------------------\n",
      "1. **Summarization Improvement**: Added explicit guidance to ensure the summary strictly contains information present in the document, addressing the issue of including extra information.\n",
      "2. **Tonality Enhancement**: Clarified the requirement to adhere to legal writing conventions, emphasizing the need for formal and complex language to match the Legalese tone. This should address the inconsistency in tone noted in the feedback.\n",
      "\n",
      "ðŸŽ¯ EXPECTED IMPROVEMENTS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. **Summarization**: By emphasizing the need to include only information present in the document, we expect the summarization score to improve significantly, potentially reaching 0.80 or higher.\n",
      "2. **Tonality**: The explicit instruction to adhere to legal writing conventions should improve the tonality score to around 0.70, as it aligns better with the expected Legalese tone.\n",
      "3. **Coherence**: Minor adjustments may also enhance coherence slightly, maintaining or slightly improving the current score of 0.90.\n",
      "4. **Safety**: The safety score is expected to remain at 1.00 as no changes were made that would affect this metric.\n",
      "\n",
      "================================================================================\n",
      "ENHANCED INSTRUCTIONS:\n",
      "================================================================================\n",
      "You are an information extraction and summarization assistant. Return output STRICTLY matching the provided JSON schema. Do not add fields. Ensure the summary is concise and succinct while comprehensively covering all major themes, arguments, examples, and key concepts from the document. The summary must be written in the specified tone, adhering strictly to the style and complexity expected of a legal document. The summary should not exceed 1000 tokens.\n",
      "\n",
      "================================================================================\n",
      "ENHANCED USER TEMPLATE:\n",
      "================================================================================\n",
      "Task: Extract metadata and summarize the document in the specified tone, ensuring adherence to legal writing conventions.\n",
      "- Tone: {tone}\n",
      "- Fields to Complete: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\n",
      "- Document: <document>{context}</document>\n",
      "\n",
      "Ensure the summary contains only information present in the document, avoiding any extrapolation or addition of new information.\n",
      "\n",
      "================================================================================\n",
      "PROMPT COMPARISON: PREVIOUS vs LATEST\n",
      "================================================================================\n",
      "\n",
      "--- PREVIOUS INSTRUCTIONS ---\n",
      "You are an information extraction and summarization assistant. Return output STRICTLY matching the provided JSON schema. Do not add fields. Summary should be concise and succint while providing comprehensive coverage of all major themes, arguments, examples, and key concepts from the document.It is important that the summary is written in the specified tone. The summary should not exceed 1000 tokens. \n",
      "\n",
      "--- LATEST INSTRUCTIONS ---\n",
      "You are an information extraction and summarization assistant. Return output STRICTLY matching the provided JSON schema. Do not add fields. Ensure the summary is concise and succinct while comprehensively covering all major themes, arguments, examples, and key concepts from the document. The summary must be written in the specified tone, adhering strictly to the style and complexity expected of a legal document. The summary should not exceed 1000 tokens.\n",
      "\n",
      "--- PREVIOUS USER TEMPLATE ---\n",
      "Task: Extract metadata and summarize the document in the specified tone.\n",
      "- Tone:{tone}\n",
      "- Fields to Complete: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\n",
      "- Document: <document>{context}</document>\n",
      "\n",
      "--- LATEST USER TEMPLATE ---\n",
      "Task: Extract metadata and summarize the document in the specified tone, ensuring adherence to legal writing conventions.\n",
      "- Tone: {tone}\n",
      "- Fields to Complete: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\n",
      "- Document: <document>{context}</document>\n",
      "\n",
      "Ensure the summary contains only information present in the document, avoiding any extrapolation or addition of new information.\n"
     ]
    }
   ],
   "source": [
    "# Execute the optimization\n",
    "print(\"=\"*80)\n",
    "print(\"LLM-POWERED PROMPT OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAnalyzing evaluation feedback and optimizing prompt...\")\n",
    "\n",
    "optimized_prompt_schema = optimize_prompt_with_llm(\n",
    "    original_prompt=prompt_list[0],\n",
    "    evaluation_results=prompt_list[0].evaluation_results,\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.5  # Lower temperature for more focused optimization\n",
    ")\n",
    "\n",
    "# Display the optimization results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ OPTIMIZATION RATIONALE:\")\n",
    "print(\"-\" * 80)\n",
    "print(optimized_prompt_schema.OptimizationRationale)\n",
    "\n",
    "print(\"\\nðŸŽ¯ EXPECTED IMPROVEMENTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(optimized_prompt_schema.ExpectedImprovements)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED INSTRUCTIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(optimized_prompt_schema.EnhancedInstructions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED USER TEMPLATE:\")\n",
    "print(\"=\"*80)\n",
    "print(optimized_prompt_schema.EnhancedUserTemplate)\n",
    "\n",
    "# Create a new PromptBuilder with the optimized prompt\n",
    "optimized_prompt = PromptBuilder(\n",
    "    instructions=optimized_prompt_schema.EnhancedInstructions,\n",
    "    user_template=optimized_prompt_schema.EnhancedUserTemplate,\n",
    "    tone=TONE,  # Same tone\n",
    "    context=prompt_list[0].context  # Same document\n",
    ")\n",
    "\n",
    "# Add to prompt list\n",
    "prompt_list.append(optimized_prompt)\n",
    "\n",
    "# Display comparison (always compare last to previous)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT COMPARISON: PREVIOUS vs LATEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- PREVIOUS INSTRUCTIONS ---\")\n",
    "print(prompt_list[0].instructions)  # Previous item\n",
    "\n",
    "print(\"\\n--- LATEST INSTRUCTIONS ---\")\n",
    "print(prompt_list[1].instructions)  # Latest item\n",
    "\n",
    "print(\"\\n--- PREVIOUS USER TEMPLATE ---\")\n",
    "print(prompt_list[0].user_template)\n",
    "\n",
    "print(\"\\n--- LATEST USER TEMPLATE ---\")\n",
    "print(prompt_list[1].user_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6073cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING SUMMARY WITH LLM-OPTIMIZED PROMPT\n",
      "================================================================================\n",
      "\n",
      "LLM-Optimized Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "In \"Managing Oneself,\" Peter F. Drucker posits that in the contemporary knowledge economy, individuals must assume the role of their own chief executive officers. Self-management is essential to navigate careers spanning decades in a landscape where companies no longer manage employees' careers. To achieve lasting success, individuals must cultivate a profound self-awareness of their strengths, weaknesses, values, and preferred work environments.\n",
      "\n",
      "Drucker introduces the concept of feedback analysis to identify strengths and weaknesses, advising individuals to focus on enhancing their strengths rather than improving areas of incompetence. He underscores the importance of understanding one's unique performance methods, whether as readers or listeners, and how these impact learning and decision-making.\n",
      "\n",
      "Values play a pivotal role in career satisfaction and effectiveness. Compatibility between personal and organizational values is crucial; otherwise, it leads to frustration and underperformance. Drucker advises individuals to identify work environments aligned with their strengths and values to transform into star performers.\n",
      "\n",
      "Further, Drucker discusses the significance of determining one's contributions, emphasizing that knowledge workers must ask what they should contribute, considering situational requirements, personal strengths, and desired results.\n",
      "\n",
      "Effective relationship management is integral to self-management. Understanding colleagues' strengths and working methods enhances collaboration. Communication is key, and taking responsibility for it fosters trust, which is foundational to modern organizational structures.\n",
      "\n",
      "Drucker also addresses the necessity for planning the second half of one's career, advocating for developing second careers or parallel careers to maintain engagement and fulfillment. This foresight helps manage life's inevitable setbacks and maintains personal and professional growth.\n",
      "\n",
      "Overall, the shift towards self-management represents a significant transformation in societal structure, as knowledge workers outlive organizations and must adapt to a mobile, dynamic work environment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "LLM-Optimized Summary (Full JSON):\n",
      "{\n",
      "  \"Author\": \"Peter F. Drucker\",\n",
      "  \"Title\": \"Managing Oneself\",\n",
      "  \"Relevance\": \"The article is critical for AI professionals as it emphasizes the importance of self-management, a key skill in dynamic fields like AI, where adaptability and continuous self-improvement are paramount. Understanding one's strengths, values, and work styles can enhance productivity and career development in the rapidly evolving AI landscape.\",\n",
      "  \"Summary\": \"In \\\"Managing Oneself,\\\" Peter F. Drucker posits that in the contemporary knowledge economy, individuals must assume the role of their own chief executive officers. Self-management is essential to navigate careers spanning decades in a landscape where companies no longer manage employees' careers. To achieve lasting success, individuals must cultivate a profound self-awareness of their strengths, weaknesses, values, and preferred work environments.\\n\\nDrucker introduces the concept of feedback analysis to identify strengths and weaknesses, advising individuals to focus on enhancing their strengths rather than improving areas of incompetence. He underscores the importance of understanding one's unique performance methods, whether as readers or listeners, and how these impact learning and decision-making.\\n\\nValues play a pivotal role in career satisfaction and effectiveness. Compatibility between personal and organizational values is crucial; otherwise, it leads to frustration and underperformance. Drucker advises individuals to identify work environments aligned with their strengths and values to transform into star performers.\\n\\nFurther, Drucker discusses the significance of determining one's contributions, emphasizing that knowledge workers must ask what they should contribute, considering situational requirements, personal strengths, and desired results.\\n\\nEffective relationship management is integral to self-management. Understanding colleagues' strengths and working methods enhances collaboration. Communication is key, and taking responsibility for it fosters trust, which is foundational to modern organizational structures.\\n\\nDrucker also addresses the necessity for planning the second half of one's career, advocating for developing second careers or parallel careers to maintain engagement and fulfillment. This foresight helps manage life's inevitable setbacks and maintains personal and professional growth.\\n\\nOverall, the shift towards self-management represents a significant transformation in societal structure, as knowledge workers outlive organizations and must adapt to a mobile, dynamic work environment.\",\n",
      "  \"Tone\": \"Formal and Legalistic\",\n",
      "  \"InputTokens\": 10900,\n",
      "  \"OutputTokens\": 443\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have at least 2 prompts before proceeding\n",
    "if len(prompt_list) < 2:\n",
    "    raise ValueError(\"Need at least 2 prompts in prompt_list\")\n",
    "\n",
    "# Generate summary using LLM-optimized prompt\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING SUMMARY WITH LLM-OPTIMIZED PROMPT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "prompt_list[-1].set_response(\n",
    "    client.responses.parse(\n",
    "        model=MODEL_NAME,\n",
    "        input=prompt_list[-1].get_input(),\n",
    "        text_format=SummarySchema,\n",
    "        temperature=0.7,  # Same temperature for fair comparison\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract and store result\n",
    "optimized_result = prompt_list[-1].response.output_parsed.model_copy(update={\n",
    "    \"InputTokens\": prompt_list[-1].response.usage.input_tokens,\n",
    "    \"OutputTokens\": prompt_list[-1].response.usage.output_tokens,\n",
    "})\n",
    "prompt_list[-1].set_result(optimized_result)\n",
    "\n",
    "# Display the optimized summary\n",
    "print(\"\\nLLM-Optimized Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(prompt_list[-1].result.Summary)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# Display full JSON\n",
    "print(\"\\nLLM-Optimized Summary (Full JSON):\")\n",
    "print(json.dumps(prompt_list[-1].result.model_dump(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3aa8089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_list length: 2\n",
      "Previous: <__main__.PromptBuilder object at 0x121ff0920>\n",
      "Latest: <__main__.PromptBuilder object at 0x121b55400>\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt_list length: {len(prompt_list)}\")\n",
    "if len(prompt_list) >= 2:\n",
    "    print(f\"Previous: {prompt_list[-2]}\")\n",
    "    print(f\"Latest: {prompt_list[-1]}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Need at least 2 items in prompt_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7179bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:17<00:52, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:21<00:18,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:26<00:07,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonality: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:28<00:00,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety: 1.00\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE COMPARISON: PREVIOUS vs LATEST\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SCORE COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Summarization:\n",
      "  Previous:  0.00\n",
      "  Latest:    0.50\n",
      "  Change:    +0.50 (+inf%)\n",
      "  âœ… Significant improvement!\n",
      "\n",
      "Coherence:\n",
      "  Previous:  0.90\n",
      "  Latest:    0.90\n",
      "  Change:    +0.00 (+0.4%)\n",
      "  âœ… Improvement\n",
      "\n",
      "Tonality:\n",
      "  Previous:  0.28\n",
      "  Latest:    0.69\n",
      "  Change:    +0.42 (+151.1%)\n",
      "  âœ… Significant improvement!\n",
      "\n",
      "Safety:\n",
      "  Previous:  1.00\n",
      "  Latest:    1.00\n",
      "  Change:    -0.00 (-0.1%)\n",
      "  âš ï¸  Regression\n",
      "\n",
      "================================================================================\n",
      "LLM PREDICTIONS vs ACTUAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Expected Improvements (from LLM):\n",
      "1. **Summarization**: By emphasizing the need to include only information present in the document, we expect the summarization score to improve significantly, potentially reaching 0.80 or higher.\n",
      "2. **Tonality**: The explicit instruction to adhere to legal writing conventions should improve the tonality score to around 0.70, as it aligns better with the expected Legalese tone.\n",
      "3. **Coherence**: Minor adjustments may also enhance coherence slightly, maintaining or slightly improving the current score of 0.90.\n",
      "4. **Safety**: The safety score is expected to remain at 1.00 as no changes were made that would affect this metric.\n",
      "\n",
      "Actual Results:\n",
      "  Summarization: +0.50 change\n",
      "  Coherence: +0.00 change\n",
      "  Tonality: +0.42 change\n",
      "  Safety: -0.00 change\n",
      "\n",
      "================================================================================\n",
      "TOKEN USAGE COMPARISON\n",
      "================================================================================\n",
      "Previous Summary:\n",
      "  Input Tokens:  10,863\n",
      "  Output Tokens: 467\n",
      "  Total:         11,330\n",
      "\n",
      "Latest Summary:\n",
      "  Input Tokens:  10,900\n",
      "  Output Tokens: 443\n",
      "  Total:         11,343\n",
      "\n",
      "Difference:\n",
      "  Input Tokens:  +37\n",
      "  Output Tokens: -24\n",
      "\n",
      "================================================================================\n",
      "PROMPT LENGTH COMPARISON\n",
      "================================================================================\n",
      "Previous prompt length: 51,049 characters\n",
      "Latest prompt length: 51,283 characters\n",
      "Difference: +234 characters\n",
      "\n",
      "================================================================================\n",
      "SIDE-BY-SIDE SUMMARY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "PREVIOUS SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "In \"Managing Oneself,\" Peter F. Drucker outlines the necessity for individuals, particularly knowledge workers, to take charge of their careers by understanding their strengths, values, and work styles. Pressed by the demands of a modern knowledge economy, professionals must act as their own CEOs, identifying areas where they can excel and make significant contributions.\n",
      "\n",
      "Drucker advocates for using feedback analysis to reveal strengths and weaknesses. This involves comparing expected outcomes of decisions with actual results over time. By focusing on strengths, individuals can achieve excellence and avoid the inefficiencies of attempting to improve weaknesses.\n",
      "\n",
      "The article stresses the importance of understanding how one performs, with distinctions made between readers and listeners, and various learning styles, such as learning by writing or doing. Knowledge of these traits enables individuals to align their work environments and tasks with their personal styles for greater effectiveness.\n",
      "\n",
      "Values are another cornerstone of self-management, where alignment between personal and organizational values is crucial to avoid frustration and underperformance. Individuals should choose environments that resonate with their personal values to enhance satisfaction and efficacy.\n",
      "\n",
      "Drucker discusses career placement, urging individuals to assess where they belong based on their strengths, performance, and values. This self-awareness allows for strategic decision-making regarding career opportunities and assignments.\n",
      "\n",
      "Finally, Drucker addresses the concept of contribution, encouraging individuals to identify what they can contribute based on situational needs and personal capabilities. This proactive approach to career management is vital in a knowledge economy where traditional corporate support structures are less prevalent.\n",
      "\n",
      "The article concludes with advice on managing the second half of one's career, suggesting options such as second careers, parallel careers, and social entrepreneurship to maintain engagement and productivity. This strategic self-management, according to Drucker, is essential for sustaining a long and fulfilling professional life.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "LATEST SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "In \"Managing Oneself,\" Peter F. Drucker posits that in the contemporary knowledge economy, individuals must assume the role of their own chief executive officers. Self-management is essential to navigate careers spanning decades in a landscape where companies no longer manage employees' careers. To achieve lasting success, individuals must cultivate a profound self-awareness of their strengths, weaknesses, values, and preferred work environments.\n",
      "\n",
      "Drucker introduces the concept of feedback analysis to identify strengths and weaknesses, advising individuals to focus on enhancing their strengths rather than improving areas of incompetence. He underscores the importance of understanding one's unique performance methods, whether as readers or listeners, and how these impact learning and decision-making.\n",
      "\n",
      "Values play a pivotal role in career satisfaction and effectiveness. Compatibility between personal and organizational values is crucial; otherwise, it leads to frustration and underperformance. Drucker advises individuals to identify work environments aligned with their strengths and values to transform into star performers.\n",
      "\n",
      "Further, Drucker discusses the significance of determining one's contributions, emphasizing that knowledge workers must ask what they should contribute, considering situational requirements, personal strengths, and desired results.\n",
      "\n",
      "Effective relationship management is integral to self-management. Understanding colleagues' strengths and working methods enhances collaboration. Communication is key, and taking responsibility for it fosters trust, which is foundational to modern organizational structures.\n",
      "\n",
      "Drucker also addresses the necessity for planning the second half of one's career, advocating for developing second careers or parallel careers to maintain engagement and fulfillment. This foresight helps manage life's inevitable setbacks and maintains personal and professional growth.\n",
      "\n",
      "Overall, the shift towards self-management represents a significant transformation in societal structure, as knowledge workers outlive organizations and must adapt to a mobile, dynamic work environment.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EVALUATION FEEDBACK COMPARISON\n",
      "================================================================================\n",
      "\n",
      "--- PREVIOUS EVALUATION FEEDBACK ---\n",
      "{\n",
      "  \"SummarizationScore\": 0.0,\n",
      "  \"SummarizationReason\": \"The score is 0.00 because the summary includes extra information that is not present in the original text, which can lead to misunderstandings and misinterpretations of the content.\",\n",
      "  \"CoherenceScore\": 0.9014194565542175,\n",
      "  \"CoherenceReason\": \"The summary presents a clear logical flow, effectively outlining Drucker's key concepts such as self-awareness, feedback analysis, and the importance of aligning personal values with organizational values. The ideas are well-connected, allowing the reader to follow the progression of thought without confusion. The organization is strong, with a logical sequence that covers strengths, performance, values, and contributions. However, while the summary is comprehensive, it could benefit from slightly more emphasis on the implications of Drucker's ideas for practical application, which would enhance its overall clarity and impact.\",\n",
      "  \"TonalityScore\": 0.2751214949183264,\n",
      "  \"TonalityReason\": \"The summary lacks a consistent Legalese tone, as it employs a more conversational and straightforward style rather than the formal and complex language typical of legal documents. While it does convey the main ideas of Drucker's work, the tone shifts to a more casual narrative at times, which detracts from the intended Legalese style. Additionally, the language style does not fully align with what would be expected for a legal summary, and a reader may not identify this summary as having the intended tone.\",\n",
      "  \"SafetyScore\": 0.9987568348380739,\n",
      "  \"SafetyReason\": \"The summary is free from harmful or offensive content and does not contain any bias, stereotypes, or discriminatory language. It is appropriate for a professional or educational context, discussing self-management strategies in a respectful and considerate manner. Additionally, it does not promote any dangerous or illegal activities, aligning well with all evaluation steps.\"\n",
      "}\n",
      "\n",
      "--- LATEST EVALUATION FEEDBACK ---\n",
      "{\n",
      "  \"SummarizationScore\": 0.5,\n",
      "  \"SummarizationReason\": \"The score is 0.50 because the summary contradicts key points from the original text regarding the focus on strengths and weaknesses, introduces extra information not present in the original, and leaves unanswered questions that the original could clarify.\",\n",
      "  \"CoherenceScore\": 0.9046649678210421,\n",
      "  \"CoherenceReason\": \"The summary presents a clear logical flow, effectively covering the key concepts from Drucker's work, such as self-management, feedback analysis, and the importance of aligning personal values with organizational values. The ideas are well-connected, allowing the reader to follow the argument without confusion. The organization of the summary is logical, with a coherent sequence that addresses the main themes of the article. However, it could benefit from slightly more emphasis on the specific methods Drucker suggests for self-management, which would enhance the completeness of the summary.\",\n",
      "  \"TonalityScore\": 0.6907339897369521,\n",
      "  \"TonalityReason\": \"The summary maintains a consistent Legalese tone throughout, effectively using formal language and structure appropriate for the content. It avoids significant tone shifts and aligns well with the expectations of a professional summary. However, while the language is generally suitable, there are moments where it could be more concise, which slightly detracts from the overall effectiveness of the Legalese tone.\",\n",
      "  \"SafetyScore\": 0.9974042641624846,\n",
      "  \"SafetyReason\": \"The summary is free from harmful, offensive, or inappropriate content and does not contain any bias, stereotypes, or discriminatory language. It is appropriate for a professional or educational context, promoting self-management and personal development without endorsing dangerous or illegal activities. The language used is respectful and considerate, aligning well with the evaluation criteria.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Evaluate the most recent prompt in prompt_list\n",
    "evaluate_prompt_builder(\n",
    "    prompt_list[-1],\n",
    "    evaluation_model=EVALUATION_MODEL,\n",
    "    summarization_threshold=0.7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Comprehensive comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON: PREVIOUS vs LATEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure we have evaluation results for both\n",
    "if prompt_list[-2].evaluation_results is None or prompt_list[-1].evaluation_results is None:\n",
    "    raise ValueError(\"Both prompts must have evaluation results to compare\")\n",
    "\n",
    "# Score comparison\n",
    "print(\"\\nðŸ“Š SCORE COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_metrics = [\n",
    "    (\"Summarization\", \n",
    "     prompt_list[-2].evaluation_results.SummarizationScore,\n",
    "     prompt_list[-1].evaluation_results.SummarizationScore),\n",
    "    (\"Coherence\",\n",
    "     prompt_list[-2].evaluation_results.CoherenceScore,\n",
    "     prompt_list[-1].evaluation_results.CoherenceScore),\n",
    "    (\"Tonality\",\n",
    "     prompt_list[-2].evaluation_results.TonalityScore,\n",
    "     prompt_list[-1].evaluation_results.TonalityScore),\n",
    "    (\"Safety\",\n",
    "     prompt_list[-2].evaluation_results.SafetyScore,\n",
    "     prompt_list[-1].evaluation_results.SafetyScore),\n",
    "]\n",
    "\n",
    "for metric_name, previous_score, latest_score in comparison_metrics:\n",
    "    improvement = latest_score - previous_score\n",
    "    improvement_pct = (improvement / previous_score * 100) if previous_score > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Previous:  {previous_score:.2f}\")\n",
    "    print(f\"  Latest:    {latest_score:.2f}\")\n",
    "    print(f\"  Change:    {improvement:+.2f} ({improvement_pct:+.1f}%)\")\n",
    "    \n",
    "    if improvement > 0.1:\n",
    "        print(f\"  âœ… Significant improvement!\")\n",
    "    elif improvement > 0:\n",
    "        print(f\"  âœ… Improvement\")\n",
    "    elif improvement < -0.1:\n",
    "        print(f\"  âš ï¸  Significant regression\")\n",
    "    elif improvement < 0:\n",
    "        print(f\"  âš ï¸  Regression\")\n",
    "    else:\n",
    "        print(f\"  âž¡ï¸  No change\")\n",
    "\n",
    "# Compare LLM's predictions vs actual results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LLM PREDICTIONS vs ACTUAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExpected Improvements (from LLM):\")\n",
    "print(optimized_prompt_schema.ExpectedImprovements)\n",
    "print(\"\\nActual Results:\")\n",
    "for metric_name, previous_score, latest_score in comparison_metrics:\n",
    "    improvement = latest_score - previous_score\n",
    "    print(f\"  {metric_name}: {improvement:+.2f} change\")\n",
    "\n",
    "# Token usage comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKEN USAGE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Previous Summary:\")\n",
    "print(f\"  Input Tokens:  {prompt_list[-2].result.InputTokens:,}\")\n",
    "print(f\"  Output Tokens: {prompt_list[-2].result.OutputTokens:,}\")\n",
    "print(f\"  Total:         {prompt_list[-2].result.InputTokens + prompt_list[-2].result.OutputTokens:,}\")\n",
    "print(f\"\\nLatest Summary:\")\n",
    "print(f\"  Input Tokens:  {prompt_list[-1].result.InputTokens:,}\")\n",
    "print(f\"  Output Tokens: {prompt_list[-1].result.OutputTokens:,}\")\n",
    "print(f\"  Total:         {prompt_list[-1].result.InputTokens + prompt_list[-1].result.OutputTokens:,}\")\n",
    "\n",
    "input_diff = prompt_list[-1].result.InputTokens - prompt_list[-2].result.InputTokens\n",
    "output_diff = prompt_list[-1].result.OutputTokens - prompt_list[-2].result.OutputTokens\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Input Tokens:  {input_diff:+,}\")\n",
    "print(f\"  Output Tokens: {output_diff:+,}\")\n",
    "\n",
    "# Prompt length comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT LENGTH COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "previous_prompt_length = len(prompt_list[-2].instructions) + len(prompt_list[-2].user_content)\n",
    "latest_prompt_length = len(prompt_list[-1].instructions) + len(prompt_list[-1].user_content)\n",
    "print(f\"Previous prompt length: {previous_prompt_length:,} characters\")\n",
    "print(f\"Latest prompt length: {latest_prompt_length:,} characters\")\n",
    "print(f\"Difference: {latest_prompt_length - previous_prompt_length:+,} characters\")\n",
    "\n",
    "# Side-by-side summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE SUMMARY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPREVIOUS SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "print(prompt_list[-2].result.Summary)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nLATEST SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "print(prompt_list[-1].result.Summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Evaluation feedback comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION FEEDBACK COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- PREVIOUS EVALUATION FEEDBACK ---\")\n",
    "print(prompt_list[-2].get_evaluation_results_json(indent=2))\n",
    "\n",
    "print(\"\\n--- LATEST EVALUATION FEEDBACK ---\")\n",
    "print(prompt_list[-1].get_evaluation_results_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5846650",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Following is not Necessary for the Assignment - \n",
    "# The purpose is to generate some number of additional iterations of prompt improvements. \n",
    "# Each iteration is on the last prompt in the list.\n",
    "\n",
    "def run_optimization_iterations(\n",
    "    prompt_list: list,\n",
    "    num_iterations: int = 5,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    evaluation_model: str = EVALUATION_MODEL,\n",
    "    temperature: float = 0.5,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run multiple optimization iterations on the prompt list.\n",
    "    \n",
    "    Args:\n",
    "        prompt_list: List of PromptBuilder objects\n",
    "        num_iterations: Number of iterations to run\n",
    "        model_name: Model for summary generation\n",
    "        evaluation_model: Model for evaluation\n",
    "        temperature: Temperature for optimization (kept constant)\n",
    "        verbose: Whether to show detailed output during iterations\n",
    "    \"\"\"\n",
    "    for iteration in range(num_iterations):\n",
    "        current_iteration = len(prompt_list)\n",
    "        \n",
    "        if not verbose:\n",
    "            print(f\"Iteration {current_iteration}/{len(prompt_list) + num_iterations - 1}...\", end=\" \", flush=True)\n",
    "        \n",
    "        # Get the latest prompt\n",
    "        current_prompt = prompt_list[-1]\n",
    "        \n",
    "        # Optimize based on evaluation results\n",
    "        optimized_schema = optimize_prompt_with_llm(\n",
    "            original_prompt=current_prompt,\n",
    "            evaluation_results=current_prompt.evaluation_results,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Create new prompt\n",
    "        new_prompt = PromptBuilder(\n",
    "            instructions=optimized_schema.EnhancedInstructions,\n",
    "            user_template=optimized_schema.EnhancedUserTemplate,\n",
    "            tone=TONE,\n",
    "            context=prompt_list[0].context\n",
    "        )\n",
    "        \n",
    "        # Generate summary\n",
    "        new_prompt.set_response(\n",
    "            client.responses.parse(\n",
    "                model=model_name,\n",
    "                input=new_prompt.get_input(),\n",
    "                text_format=SummarySchema,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        new_result = new_prompt.response.output_parsed.model_copy(update={\n",
    "            \"InputTokens\": new_prompt.response.usage.input_tokens,\n",
    "            \"OutputTokens\": new_prompt.response.usage.output_tokens,\n",
    "        })\n",
    "        new_prompt.set_result(new_result)\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluate_prompt_builder(\n",
    "            new_prompt,\n",
    "            evaluation_model=evaluation_model,\n",
    "            summarization_threshold=0.7,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Append to list\n",
    "        prompt_list.append(new_prompt)\n",
    "        \n",
    "        if not verbose:\n",
    "            print(\"âœ“\")\n",
    "    \n",
    "    return prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd46f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING 5 ADDITIONAL OPTIMIZATION ITERATIONS\n",
      "================================================================================\n",
      "\n",
      "Iteration 7/11... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[280]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrun_optimization_iterations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll iterations complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[279]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrun_optimization_iterations\u001b[39m\u001b[34m(prompt_list, num_iterations, model_name, evaluation_model, temperature, verbose)\u001b[39m\n\u001b[32m     27\u001b[39m current_prompt = prompt_list[-\u001b[32m1\u001b[39m]\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Optimize based on evaluation results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m optimized_schema = \u001b[43moptimize_prompt_with_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_prompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Create new prompt\u001b[39;00m\n\u001b[32m     38\u001b[39m new_prompt = PromptBuilder(\n\u001b[32m     39\u001b[39m     instructions=optimized_schema.EnhancedInstructions,\n\u001b[32m     40\u001b[39m     user_template=optimized_schema.EnhancedUserTemplate,\n\u001b[32m     41\u001b[39m     tone=TONE,\n\u001b[32m     42\u001b[39m     context=prompt_list[\u001b[32m0\u001b[39m].context\n\u001b[32m     43\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[271]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36moptimize_prompt_with_llm\u001b[39m\u001b[34m(original_prompt, evaluation_results, model_name, temperature)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Call the LLM to optimize\u001b[39;00m\n\u001b[32m    118\u001b[39m optimization_messages = [\n\u001b[32m    119\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt_optimizer_instructions},\n\u001b[32m    120\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: optimization_prompt_content}\n\u001b[32m    121\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43moptimization_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOptimizedPrompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.output_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/resources/responses/responses.py:1129\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(self, text_format, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, verbosity, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sites/dsi/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Disabled Additional Prompt Runs for Uploading to Github\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING 5 ADDITIONAL OPTIMIZATION ITERATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# run_optimization_iterations(\n",
    "#     prompt_list=prompt_list,\n",
    "#     num_iterations=5,\n",
    "#     temperature=0.5,\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "print()\n",
    "print(\"All iterations complete!\")\n",
    "print(f\"Total iterations: {len(prompt_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae864f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROGRESSION ANALYSIS: ALL ITERATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SCORE PROGRESSION TABLE:\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration    Summ     Coh      Tone     Safety   Out Tokens  \n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 0     0.00     0.90     0.28     1.00          467\n",
      "Iteration 1     0.50     0.90     0.69     1.00          443\n",
      "Iteration 2     0.25     0.89     0.83     1.00          486\n",
      "Iteration 3     0.75     0.90     0.44     1.00          411\n",
      "Iteration 4     0.00     0.90     0.84     1.00          496\n",
      "Iteration 5     0.00     0.89     0.80     1.00          396\n",
      "Iteration 6     0.80     0.89     0.30     1.00          439\n",
      "\n",
      "ðŸ“ˆ SCORE CHANGES (First â†’ Last):\n",
      "--------------------------------------------------------------------------------\n",
      "Summarization   0.00 â†’ 0.80  (+0.80, +inf%)\n",
      "Coherence       0.90 â†’ 0.89  (-0.01, -1.6%)\n",
      "Tonality        0.28 â†’ 0.30  (+0.02, +9.0%)\n",
      "Safety          1.00 â†’ 1.00  (-0.00, -0.1%)\n",
      "\n",
      "ðŸ’° TOKEN USAGE PROGRESSION:\n",
      "--------------------------------------------------------------------------------\n",
      "First iteration:  11,330 tokens\n",
      "Last iteration:   11,542 tokens\n",
      "Difference:       +212 tokens\n",
      "\n",
      "ðŸ“ SUMMARY COMPARISON: FIRST vs LAST\n",
      "================================================================================\n",
      "\n",
      "FIRST ITERATION (Original):\n",
      "--------------------------------------------------------------------------------\n",
      "In \"Managing Oneself,\" Peter F. Drucker outlines the necessity for individuals, particularly knowledge workers, to take charge of their careers by understanding their strengths, values, and work styles. Pressed by the demands of a modern knowledge economy, professionals must act as their own CEOs, identifying areas where they can excel and make significant contributions.\n",
      "\n",
      "Drucker advocates for using feedback analysis to reveal strengths and weaknesses. This involves comparing expected outcomes of decisions with actual results over time. By focusing on strengths, individuals can achieve excellence and avoid the inefficiencies of attempting to improve weaknesses.\n",
      "\n",
      "The article stresses the importance of understanding how one performs, with distinctions made between readers and listeners, and various learning styles, such as learning by writing or doing. Knowledge of these traits enables individuals to align their work environments and tasks with their personal styles for greater effectiveness.\n",
      "\n",
      "Values are another cornerstone of self-management, where alignment between personal and organizational values is crucial to avoid frustration and underperformance. Individuals should choose environments that resonate with their personal values to enhance satisfaction and efficacy.\n",
      "\n",
      "Drucker discusses career placement, urging individuals to assess where they belong based on their strengths, performance, and values. This self-awareness allows for strategic decision-making regarding career opportunities and assignments.\n",
      "\n",
      "Finally, Drucker addresses the concept of contribution, encouraging individuals to identify what they can contribute based on situational needs and personal capabilities. This proactive approach to career management is vital in a knowledge economy where traditional corporate support structures are less prevalent.\n",
      "\n",
      "The article concludes with advice on managing the second half of one's career, suggesting options such as second careers, parallel careers, and social entrepreneurship to maintain engagement and productivity. This strategic self-management, according to Drucker, is essential for sustaining a long and fulfilling professional life.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "LAST ITERATION (After 6 optimizations):\n",
      "--------------------------------------------------------------------------------\n",
      "The article 'Managing Oneself,' authored by Peter F. Drucker, elucidates the necessity for individuals, particularly knowledge workers, to assume responsibility for self-management in a modern professional setting. It underscores the imperative of knowing one's strengths, values, and working style to achieve excellence and satisfaction over a potentially extended career lifespan. Drucker emphasizes the use of feedback analysis as a pivotal methodology for identifying personal strengths and weaknesses. This process involves documenting expected outcomes of decisions and comparing them with actual results over time, thereby uncovering consistent patterns that reveal areas of competence and deficiency. \n",
      "\n",
      "Drucker further interrogates the questions critical for self-assessment: 'What are my strengths?' 'How do I perform?' 'What are my values?' 'Where do I belong?' and 'What can I contribute?' Each question is designed to guide individuals in aligning their professional endeavors with their innate abilities and ethical convictions, ensuring productivity and fulfillment. The article also highlights the importance of understanding one's learning style, whether as a reader or a listener, and adapting work strategies accordingly. \n",
      "\n",
      "Moreover, Drucker posits that individuals should consider the values of their organizations to avoid conflict and frustration. The discourse extends to planning the second half of one's career, advocating for the development of a second or parallel career as a means to maintain engagement and purpose. This involves preparing for future shifts well before reaching mid-career.\n",
      "\n",
      "Finally, Drucker discusses the obligation of managing relationships within professional environments, advocating for clear communication of one's strengths, work habits, and values to ensure effective collaboration. He concludes with a reflection on the transformation of societal structures due to the shift from manual to knowledge work, necessitating a revolutionary approach to self-management in this era.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ† BEST SCORES ACHIEVED:\n",
      "--------------------------------------------------------------------------------\n",
      "Summarization: 0.80 (Iteration 6)\n",
      "Coherence:     0.90 (Iteration 1)\n",
      "Tonality:      0.84 (Iteration 4)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def display_progression_analysis(prompt_list: list):\n",
    "    \"\"\"Display comprehensive analysis across all iterations.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PROGRESSION ANALYSIS: ALL ITERATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Scores table\n",
    "    print(\"\\nðŸ“Š SCORE PROGRESSION TABLE:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Iteration':<12} {'Summ':<8} {'Coh':<8} {'Tone':<8} {'Safety':<8} {'Out Tokens':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, prompt in enumerate(prompt_list):\n",
    "        if prompt.evaluation_results:\n",
    "            print(f\"Iteration {i:<3} \"\n",
    "                  f\"{prompt.evaluation_results.SummarizationScore:>6.2f}   \"\n",
    "                  f\"{prompt.evaluation_results.CoherenceScore:>6.2f}   \"\n",
    "                  f\"{prompt.evaluation_results.TonalityScore:>6.2f}   \"\n",
    "                  f\"{prompt.evaluation_results.SafetyScore:>6.2f}   \"\n",
    "                  f\"{prompt.result.OutputTokens:>10,}\")\n",
    "    \n",
    "    # Score improvements\n",
    "    print(\"\\nðŸ“ˆ SCORE CHANGES (First â†’ Last):\")\n",
    "    print(\"-\" * 80)\n",
    "    first = prompt_list[0].evaluation_results\n",
    "    last = prompt_list[-1].evaluation_results\n",
    "    \n",
    "    metrics = [\n",
    "        (\"Summarization\", first.SummarizationScore, last.SummarizationScore),\n",
    "        (\"Coherence\", first.CoherenceScore, last.CoherenceScore),\n",
    "        (\"Tonality\", first.TonalityScore, last.TonalityScore),\n",
    "        (\"Safety\", first.SafetyScore, last.SafetyScore),\n",
    "    ]\n",
    "    \n",
    "    for metric_name, first_score, last_score in metrics:\n",
    "        change = last_score - first_score\n",
    "        change_pct = (change / first_score * 100) if first_score > 0 else float('inf')\n",
    "        print(f\"{metric_name:<15} {first_score:.2f} â†’ {last_score:.2f}  \"\n",
    "              f\"({change:+.2f}, {change_pct:+.1f}%)\")\n",
    "    \n",
    "    # Token progression\n",
    "    print(\"\\nðŸ’° TOKEN USAGE PROGRESSION:\")\n",
    "    print(\"-\" * 80)\n",
    "    first_total = prompt_list[0].result.InputTokens + prompt_list[0].result.OutputTokens\n",
    "    last_total = prompt_list[-1].result.InputTokens + prompt_list[-1].result.OutputTokens\n",
    "    print(f\"First iteration:  {first_total:,} tokens\")\n",
    "    print(f\"Last iteration:   {last_total:,} tokens\")\n",
    "    print(f\"Difference:       {last_total - first_total:+,} tokens\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(\"\\nðŸ“ SUMMARY COMPARISON: FIRST vs LAST\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nFIRST ITERATION (Original):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(prompt_list[0].result.Summary)\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"\\nLAST ITERATION (After {len(prompt_list)-1} optimizations):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(prompt_list[-1].result.Summary)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Best scores achievedd\n",
    "    print(\"\\nðŸ† BEST SCORES ACHIEVED:\")\n",
    "    print(\"-\" * 80)\n",
    "    best_summ = max(enumerate(prompt_list), key=lambda x: x[1].evaluation_results.SummarizationScore)\n",
    "    best_coh = max(enumerate(prompt_list), key=lambda x: x[1].evaluation_results.CoherenceScore)\n",
    "    best_tone = max(enumerate(prompt_list), key=lambda x: x[1].evaluation_results.TonalityScore)\n",
    "    \n",
    "    print(f\"Summarization: {best_summ[1].evaluation_results.SummarizationScore:.2f} (Iteration {best_summ[0]})\")\n",
    "    print(f\"Coherence:     {best_coh[1].evaluation_results.CoherenceScore:.2f} (Iteration {best_coh[0]})\")\n",
    "    print(f\"Tonality:      {best_tone[1].evaluation_results.TonalityScore:.2f} (Iteration {best_tone[0]})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Display the analysis\n",
    "display_progression_analysis(prompt_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
