{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets_grassriots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS: Text Cleaning Helper Functions for LLM Processing.\n",
    "import re\n",
    "def clean_document_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize document text for LLM processing.\n",
    "    \n",
    "    This function applies basic data cleaning techniques to prepare PDF-extracted\n",
    "    text for optimal LLM processing. It handles common issues like encoding errors,\n",
    "    excessive whitespace, and improper line breaks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw document text extracted from PDF\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and normalized text ready for LLM processing\n",
    "    \"\"\"\n",
    "    # Step 1: Handle encoding issues gracefully\n",
    "    # Remove or replace problematic characters that may cause encoding errors\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # Handle common encoding issues by removing problematic unicode characters\n",
    "    text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # Step 2: Normalize line breaks - preserve paragraph breaks but join broken sentences\n",
    "    # Replace double newlines (paragraph breaks) with a temporary marker\n",
    "    text = text.replace('\\n\\n', '|||PARAGRAPH_BREAK|||')\n",
    "    # Replace single newlines with spaces (these are likely broken sentences)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Restore paragraph breaks\n",
    "    text = text.replace('|||PARAGRAPH_BREAK|||', '\\n\\n')\n",
    "    \n",
    "    # Step 3: Normalize whitespace\n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Step 4: Clean up hyphenated line breaks (common in PDFs)\n",
    "    # Fix words broken across lines with hyphens followed by space (e.g., \"word- \\nword\" -> \"wordword\")\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Step 5: Remove leading and trailing whitespace from each line\n",
    "    lines = text.split('\\n')\n",
    "    lines = [line.strip() for line in lines]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    # Step 6: Remove excessive blank lines (more than 2 consecutive newlines)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Step 7: Final trim of leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS: Text Cleaning Evaluation Helper Functions - DO NOT MODIFY\n",
    "try:\n",
    "    import tiktoken\n",
    "    TOKENIZER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TOKENIZER_AVAILABLE = False\n",
    "    print(\"Note: tiktoken not available. Using word-based token approximation.\")\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Count tokens in text using tiktoken if available, otherwise approximate.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to count tokens for\n",
    "        model (str): Model name for tokenizer (default: gpt-4o-mini)\n",
    "        \n",
    "    Returns:\n",
    "        int: Approximate token count\n",
    "    \"\"\"\n",
    "    if TOKENIZER_AVAILABLE:\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(model)\n",
    "            return len(encoding.encode(text))\n",
    "        except:\n",
    "            # Fallback to word-based approximation\n",
    "            return len(text.split()) // 0.75  # Rough approximation: ~0.75 words per token\n",
    "    else:\n",
    "        # Simple approximation: average English word is ~1.3 tokens\n",
    "        return int(len(text.split()) * 1.3)\n",
    "\n",
    "def evaluate_cleaning(original_text, cleaned_text):\n",
    "    \"\"\"\n",
    "    Evaluate the effectiveness of document cleaning by comparing original vs cleaned text.\n",
    "    \n",
    "    This function computes quantitative metrics including text statistics, whitespace\n",
    "    reduction, and token efficiency to demonstrate cleaning effectiveness.\n",
    "    \n",
    "    Args:\n",
    "        original_text (str): Original text before cleaning\n",
    "        cleaned_text (str): Text after cleaning\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Helper function to count patterns\n",
    "    def count_pattern(text, pattern):\n",
    "        return len(re.findall(pattern, text))\n",
    "    \n",
    "    # Helper function to count_multiple_spaces\n",
    "    def count_multiple_spaces(text):\n",
    "        return len(re.findall(r' {2,}', text))\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    metrics = {\n",
    "        'original': {},\n",
    "        'cleaned': {},\n",
    "        'improvements': {}\n",
    "    }\n",
    "    \n",
    "    # Character and word counts\n",
    "    metrics['original']['char_count'] = len(original_text)\n",
    "    metrics['cleaned']['char_count'] = len(cleaned_text)\n",
    "    metrics['improvements']['char_reduction'] = metrics['original']['char_count'] - metrics['cleaned']['char_count']\n",
    "    metrics['improvements']['char_reduction_pct'] = (metrics['improvements']['char_reduction'] / metrics['original']['char_count'] * 100) if metrics['original']['char_count'] > 0 else 0\n",
    "    \n",
    "    metrics['original']['word_count'] = len(original_text.split())\n",
    "    metrics['cleaned']['word_count'] = len(cleaned_text.split())\n",
    "    \n",
    "    # Whitespace metrics\n",
    "    metrics['original']['spaces'] = original_text.count(' ')\n",
    "    metrics['cleaned']['spaces'] = cleaned_text.count(' ')\n",
    "    \n",
    "    metrics['original']['tabs'] = original_text.count('\\t')\n",
    "    metrics['cleaned']['tabs'] = cleaned_text.count('\\t')\n",
    "    \n",
    "    metrics['original']['newlines'] = original_text.count('\\n')\n",
    "    metrics['cleaned']['newlines'] = cleaned_text.count('\\n')\n",
    "    \n",
    "    metrics['original']['multiple_spaces'] = count_multiple_spaces(original_text)\n",
    "    metrics['cleaned']['multiple_spaces'] = count_multiple_spaces(cleaned_text)\n",
    "    \n",
    "    # Count excessive blank lines (>2 consecutive)\n",
    "    metrics['original']['excessive_blank_lines'] = count_pattern(original_text, r'\\n{3,}')\n",
    "    metrics['cleaned']['excessive_blank_lines'] = count_pattern(cleaned_text, r'\\n{3,}')\n",
    "    \n",
    "    # Count single newlines (likely broken sentences)\n",
    "    single_newlines_original = count_pattern(original_text, r'(?<!\\n)\\n(?!\\n)')\n",
    "    single_newlines_cleaned = count_pattern(cleaned_text, r'(?<!\\n)\\n(?!\\n)')\n",
    "    metrics['original']['single_newlines'] = single_newlines_original\n",
    "    metrics['cleaned']['single_newlines'] = single_newlines_cleaned\n",
    "    \n",
    "    # Text density (non-whitespace ratio)\n",
    "    metrics['original']['text_density'] = len(re.sub(r'\\s', '', original_text)) / len(original_text) if len(original_text) > 0 else 0\n",
    "    metrics['cleaned']['text_density'] = len(re.sub(r'\\s', '', cleaned_text)) / len(cleaned_text) if len(cleaned_text) > 0 else 0\n",
    "    \n",
    "    # Sentence count (approximate)\n",
    "    metrics['original']['sentence_count'] = len(re.findall(r'[.!?]+', original_text))\n",
    "    metrics['cleaned']['sentence_count'] = len(re.findall(r'[.!?]+', cleaned_text))\n",
    "    \n",
    "    # Average words per sentence\n",
    "    metrics['original']['avg_words_per_sentence'] = metrics['original']['word_count'] / metrics['original']['sentence_count'] if metrics['original']['sentence_count'] > 0 else 0\n",
    "    metrics['cleaned']['avg_words_per_sentence'] = metrics['cleaned']['word_count'] / metrics['cleaned']['sentence_count'] if metrics['cleaned']['sentence_count'] > 0 else 0\n",
    "    \n",
    "    # Token counts\n",
    "    metrics['original']['token_count'] = count_tokens(original_text)\n",
    "    metrics['cleaned']['token_count'] = count_tokens(cleaned_text)\n",
    "    metrics['improvements']['token_reduction'] = metrics['original']['token_count'] - metrics['cleaned']['token_count']\n",
    "    metrics['improvements']['token_reduction_pct'] = (metrics['improvements']['token_reduction'] / metrics['original']['token_count'] * 100) if metrics['original']['token_count'] > 0 else 0\n",
    "    \n",
    "    # Calculate improvements\n",
    "    metrics['improvements']['spaces_reduced'] = metrics['original']['spaces'] - metrics['cleaned']['spaces']\n",
    "    metrics['improvements']['tabs_removed'] = metrics['original']['tabs'] - metrics['cleaned']['tabs']\n",
    "    metrics['improvements']['single_newlines_fixed'] = metrics['original']['single_newlines'] - metrics['cleaned']['single_newlines']\n",
    "    metrics['improvements']['multiple_spaces_reduced'] = metrics['original']['multiple_spaces'] - metrics['cleaned']['multiple_spaces']\n",
    "    metrics['improvements']['excessive_blank_lines_removed'] = metrics['original']['excessive_blank_lines'] - metrics['cleaned']['excessive_blank_lines']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_evaluation_report(metrics):\n",
    "    \"\"\"\n",
    "    Print a formatted evaluation report showing cleaning effectiveness.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Metrics dictionary from evaluate_cleaning()\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DOCUMENT CLEANING EVALUATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Text Statistics\n",
    "    print(\"ðŸ“Š TEXT STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Character Count:\")\n",
    "    print(f\"  Original: {metrics['original']['char_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['char_count']:,}\")\n",
    "    print(f\"  Reduction: {metrics['improvements']['char_reduction']:,} ({metrics['improvements']['char_reduction_pct']:.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Word Count:\")\n",
    "    print(f\"  Original: {metrics['original']['word_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['word_count']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Sentence Count:\")\n",
    "    print(f\"  Original: {metrics['original']['sentence_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['sentence_count']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Average Words per Sentence:\")\n",
    "    print(f\"  Original: {metrics['original']['avg_words_per_sentence']:.2f}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['avg_words_per_sentence']:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Whitespace Metrics\n",
    "    print(\"ðŸ”¤ WHITESPACE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Spaces:\")\n",
    "    print(f\"  Original: {metrics['original']['spaces']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['spaces']:,}\")\n",
    "    print(f\"  Reduced:  {metrics['improvements']['spaces_reduced']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Multiple Consecutive Spaces:\")\n",
    "    print(f\"  Original: {metrics['original']['multiple_spaces']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['multiple_spaces']:,}\")\n",
    "    print(f\"  Reduced:  {metrics['improvements']['multiple_spaces_reduced']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Tab Characters:\")\n",
    "    print(f\"  Original: {metrics['original']['tabs']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['tabs']:,}\")\n",
    "    print(f\"  Removed:  {metrics['improvements']['tabs_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Single Newlines (broken sentences):\")\n",
    "    print(f\"  Original: {metrics['original']['single_newlines']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['single_newlines']:,}\")\n",
    "    print(f\"  Fixed:    {metrics['improvements']['single_newlines_fixed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Excessive Blank Lines (>2 consecutive):\")\n",
    "    print(f\"  Original: {metrics['original']['excessive_blank_lines']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['excessive_blank_lines']:,}\")\n",
    "    print(f\"  Removed:  {metrics['improvements']['excessive_blank_lines_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Text Quality\n",
    "    print(\"âœ¨ TEXT QUALITY INDICATORS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Text Density (non-whitespace ratio):\")\n",
    "    print(f\"  Original: {metrics['original']['text_density']:.4f}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['text_density']:.4f}\")\n",
    "    print(f\"  Improvement: {'Higher is better - more content, less whitespace' if metrics['cleaned']['text_density'] > metrics['original']['text_density'] else 'Same or lower'}\")\n",
    "    print()\n",
    "    \n",
    "    # Token Efficiency\n",
    "    print(\"ðŸŽ¯ LLM PROCESSING EFFICIENCY\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Token Count (approximate):\")\n",
    "    print(f\"  Original: {metrics['original']['token_count']:,}\")\n",
    "    print(f\"  Cleaned:  {metrics['cleaned']['token_count']:,}\")\n",
    "    print(f\"  Reduction: {metrics['improvements']['token_reduction']:,} ({metrics['improvements']['token_reduction_pct']:.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"âœ… Characters reduced: {metrics['improvements']['char_reduction']:,} ({metrics['improvements']['char_reduction_pct']:.2f}%)\")\n",
    "    print(f\"âœ… Tokens reduced: {metrics['improvements']['token_reduction']:,} ({metrics['improvements']['token_reduction_pct']:.2f}%)\")\n",
    "    print(f\"âœ… Broken sentences fixed: {metrics['improvements']['single_newlines_fixed']:,}\")\n",
    "    print(f\"âœ… Multiple spaces normalized: {metrics['improvements']['multiple_spaces_reduced']:,}\")\n",
    "    print(f\"âœ… Tabs converted to spaces: {metrics['improvements']['tabs_removed']:,}\")\n",
    "    print(f\"âœ… Excessive blank lines removed: {metrics['improvements']['excessive_blank_lines_removed']:,}\")\n",
    "    print()\n",
    "    \n",
    "    if metrics['improvements']['token_reduction'] > 0:\n",
    "        print(f\"ðŸ’¡ The cleaned text uses {metrics['improvements']['token_reduction_pct']:.2f}% fewer tokens,\")\n",
    "        print(f\"   which means lower processing costs and faster LLM responses!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "032fc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import POF Using LangChain Library (Peter Drucker - Managing Oneself)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "# Store original text before cleaning for comparison\n",
    "original_document_text = document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cec19f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT CLEANING EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TEXT STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Character Count:\n",
      "  Original: 51,452\n",
      "  Cleaned:  50,434\n",
      "  Reduction: 1,018 (1.98%)\n",
      "\n",
      "Word Count:\n",
      "  Original: 8,670\n",
      "  Cleaned:  8,427\n",
      "\n",
      "Sentence Count:\n",
      "  Original: 578\n",
      "  Cleaned:  578\n",
      "\n",
      "Average Words per Sentence:\n",
      "  Original: 15.00\n",
      "  Cleaned:  14.58\n",
      "\n",
      "ðŸ”¤ WHITESPACE METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "Spaces:\n",
      "  Original: 7,759\n",
      "  Cleaned:  8,426\n",
      "  Reduced:  -667\n",
      "\n",
      "Multiple Consecutive Spaces:\n",
      "  Original: 5\n",
      "  Cleaned:  0\n",
      "  Reduced:  5\n",
      "\n",
      "Tab Characters:\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "Single Newlines (broken sentences):\n",
      "  Original: 1,442\n",
      "  Cleaned:  0\n",
      "  Fixed:    1,442\n",
      "\n",
      "Excessive Blank Lines (>2 consecutive):\n",
      "  Original: 0\n",
      "  Cleaned:  0\n",
      "  Removed:  0\n",
      "\n",
      "âœ¨ TEXT QUALITY INDICATORS\n",
      "--------------------------------------------------------------------------------\n",
      "Text Density (non-whitespace ratio):\n",
      "  Original: 0.8212\n",
      "  Cleaned:  0.8329\n",
      "  Improvement: Higher is better - more content, less whitespace\n",
      "\n",
      "ðŸŽ¯ LLM PROCESSING EFFICIENCY\n",
      "--------------------------------------------------------------------------------\n",
      "Token Count (approximate):\n",
      "  Original: 12,138\n",
      "  Cleaned:  10,523\n",
      "  Reduction: 1,615 (13.31%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "âœ… Characters reduced: 1,018 (1.98%)\n",
      "âœ… Tokens reduced: 1,615 (13.31%)\n",
      "âœ… Broken sentences fixed: 1,442\n",
      "âœ… Multiple spaces normalized: 5\n",
      "âœ… Tabs converted to spaces: 0\n",
      "âœ… Excessive blank lines removed: 0\n",
      "\n",
      "ðŸ’¡ The cleaned text uses 13.31% fewer tokens,\n",
      "   which means lower processing costs and faster LLM responses!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Clean the document text before processing with LLM and Output the results\n",
    "document_text = clean_document_text(document_text)\n",
    "\n",
    "# Evaluate the cleaning effectiveness\n",
    "cleaning_metrics = evaluate_cleaning(original_document_text, document_text)\n",
    "print_evaluation_report(cleaning_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 1) Output Schema - simplified for strict mode\n",
    "class SummarySchema(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid')\n",
    "    \n",
    "    Author: str = Field(..., description=\"Author of the article\")\n",
    "    Title: str = Field(..., description=\"Title of the article\")\n",
    "    Relevance: str = Field(..., description=\"One paragraph on relevance to AI professionals\")\n",
    "    Summary: str = Field(..., description=\"Concise summary, <=1000 tokens\")\n",
    "    Tone: str = Field(..., description=\"The tone used to produce the summary\")\n",
    "    InputTokens: Optional[int] = Field(default=0, description=\"Token count - DO NOT FILL, will be set by response object\")\n",
    "    OutputTokens: Optional[int] = Field(default=0, description=\"Token count - DO NOT FILL, will be by response object\")\n",
    "\n",
    "# 2) Model + Tone\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "TONE = \"Humourous\"\n",
    "\n",
    "# 3) Prompts (separated) + dynamic context injection\n",
    "instructions = (\n",
    "    \"You are an information extraction and summarization assistant. \"\n",
    "    \"Return output STRICTLY matching the provided JSON schema. Do not add fields. \"\n",
    "    \"Summary should be concise and succint, with limited commentary.\"\n",
    "    \"Write the Summary in the specified Tone and keep it under 1000 tokens. \"\n",
    "    \"Use only facts from the provided document; avoid speculation or hallucinations.\"\n",
    ")\n",
    "\n",
    "user_template = (\n",
    "    \"Task: Extract metadata and summarize the document in the specified tone.\\n\"\n",
    "    \"- Tone: {tone}\\n\"\n",
    "    \"- Fields to fill: Author, Title, Relevance (<=1 paragraph), Summary (<=1000 tokens).\\n\"\n",
    "    \"Document follows between <<< >>>. Use only its content.\\n\"\n",
    "    \"<<<\\n{context}\\n>>>\"\n",
    ")\n",
    "user_content = user_template.format(tone=TONE, context=document_text)\n",
    "\n",
    "# 4) Call the model with structured output via Chat Completions API\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=MODEL_NAME,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    text_format=SummarySchema,  # Direct Pydantic model\n",
    "    # max_completion_tokens=1200,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e30b7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, self-management! The task of being your own boss while resisting the urge to eat donuts at 3 PM. Peter F. Drucker, in his wise (and amusing) article \"Managing Oneself,\" argues that in the knowledge economy, we must all be our own CEO. Knowledge workers like us need to peel back the layers of the metaphorical onion (sorry, not sorry) to truly understand our strengths, weaknesses, and what environment makes us shine brighter than a freshly polished Apple product. The gist? Donâ€™t waste time obsessing over weaknesses. Instead, double down on what you do bestâ€”like a cat that has mastered the art of napping, or a dog that can valiantly chase its tail for hours. Drucker emphasizes the importance of feedback analysisâ€”write down your expectations, then see if reality gives you a thumbs-up or slaps your face with a metaphorical pie. Predetermined roles are out; choose your adventures based on the skills youâ€™ve honedâ€”whether you read or listen better is critical, as Eisenhower's press blunders so humorously illustrate. Understand your values, as they could be the compass in your turbulent career waters (or at least guide you to better office snacks). In short, self-knowledge is your golden ticket, and Drucker playfully nudges us towards it, insisting that our careers can become as fulfilling as a perfect slice of pizza, provided we ask the right questions and manage our contributions like the savvy professionals we aspire to be. Remember, the real fun begins when you know yourself well enough to say â€˜yesâ€™ to the right opportunities and â€˜noâ€™ to those pesky pitfalls!\n",
      "\n",
      "\n",
      "Our Completed Schema Ouput (result):\n",
      "{\n",
      "  \"Author\": \"Peter F. Drucker\",\n",
      "  \"Title\": \"Managing Oneself\",\n",
      "  \"Relevance\": \"In an era where self-management is crucial for career success, Druckerâ€™s insights are invaluable for AI professionals looking to carve out their niche in this rapidly evolving field. Understanding personal strengths, values, and performance modes helps in navigating not just technology but also the complex human interactions within organizations.\",\n",
      "  \"Summary\": \"Ah, self-management! The task of being your own boss while resisting the urge to eat donuts at 3 PM. Peter F. Drucker, in his wise (and amusing) article \\\"Managing Oneself,\\\" argues that in the knowledge economy, we must all be our own CEO. Knowledge workers like us need to peel back the layers of the metaphorical onion (sorry, not sorry) to truly understand our strengths, weaknesses, and what environment makes us shine brighter than a freshly polished Apple product. The gist? Donâ€™t waste time obsessing over weaknesses. Instead, double down on what you do bestâ€”like a cat that has mastered the art of napping, or a dog that can valiantly chase its tail for hours. Drucker emphasizes the importance of feedback analysisâ€”write down your expectations, then see if reality gives you a thumbs-up or slaps your face with a metaphorical pie. Predetermined roles are out; choose your adventures based on the skills youâ€™ve honedâ€”whether you read or listen better is critical, as Eisenhower's press blunders so humorously illustrate. Understand your values, as they could be the compass in your turbulent career waters (or at least guide you to better office snacks). In short, self-knowledge is your golden ticket, and Drucker playfully nudges us towards it, insisting that our careers can become as fulfilling as a perfect slice of pizza, provided we ask the right questions and manage our contributions like the savvy professionals we aspire to be. Remember, the real fun begins when you know yourself well enough to say â€˜yesâ€™ to the right opportunities and â€˜noâ€™ to those pesky pitfalls!\",\n",
      "  \"Tone\": \"Humourous\",\n",
      "  \"InputTokens\": 10857,\n",
      "  \"OutputTokens\": 428\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the response object in a human readable format\n",
    "# print(f\"The Complete Response Object (reponse):\\n{json.dumps(response.model_dump(), indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "# Add the token counts to the response and store the result in its own variable\n",
    "result = response.output_parsed.model_copy(update={\n",
    "    \"InputTokens\": response.usage.input_tokens,\n",
    "    \"OutputTokens\": response.usage.output_tokens,\n",
    "})\n",
    "\n",
    "# Output our summary for human review \n",
    "print(result.Summary)\n",
    "print(\"\\n\")\n",
    "# Output just the result in a human readable format\n",
    "print(f\"Our Completed Schema Ouput (result):\\n{json.dumps(result.model_dump(), indent=2, ensure_ascii=False)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
